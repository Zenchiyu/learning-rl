# -*- coding: utf-8 -*-
"""
@author: Stephane Liem Nguyen

In this file we write functions used for replicating experiments
from Sutton and Barto's book for the nonstationary variant of the 10-armed
testbed.

All the code is the same as for the stationary case except that
we added the possibility of plugging any environment being a subclass
of MultiArmedBanditEnv (using it a bit as an interface informally).

As noticeable difference, we also added a possiblity to estimate the reward
baseline using a constant step size alpha (if want to have even more control,
should separate using another variable, for instance named beta). The argument
"baseline" from single_run function is no longer boolean, it's now an integer
so we can able to select what we previously explained.

For more comments, refer to the file with the same name from 10-armed bandits.
We removed most of the  comments for the similar parts.
"""
from typing import Union
from environment.NonStationaryTestBedEnv import *
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

def argmax_indices(Q: np.ndarray):
    return np.argwhere(Q == np.max(Q)).flatten()

def eps_greedy(epsilon: float, Q: np.ndarray):
    if np.random.uniform() < epsilon:
        return np.random.choice(np.arange(Q.size))
    else:
        return np.random.choice(argmax_indices(Q))

def ucb(Q: np.ndarray, N: np.ndarray, c: Union[int, float], t: int):
    assert Q.shape == N.shape, "Q should be of same shape as N"
    U = c*np.sqrt(np.log(t + 1)/(N + np.finfo(float).eps))
    return np.random.choice(argmax_indices(Q + U))

def softmax(action_preferences):
    return np.exp(action_preferences)/np.sum(np.exp(action_preferences))

def gradient_bandit(action_preferences: np.ndarray):
    return np.random.choice(np.arange(action_preferences.size), p=softmax(action_preferences))

def get_action(action_selection_method: str="eps-greedy", **kwargs):
    if action_selection_method == "eps-greedy":
        action = eps_greedy(kwargs["epsilon"], kwargs["Q"])
    elif action_selection_method == "ucb":
        action = ucb(kwargs["Q"], kwargs["N"], kwargs["c"], kwargs["t"])
    elif action_selection_method == "gradient bandit":
        action = gradient_bandit(kwargs["action_preferences"])
    else:
        raise Exception("Action selection method (strategy, policy) does not exist.")
    return action
            
def single_run(env: MultiArmedBanditEnv, horizon: int, loc: float=0, estimation_method: str="sample-avg", alpha: float=0.1, Q_init: Union[float, int, np.ndarray]=0, action_selection_method: str="eps-greedy", epsilon: float=0.1, c: Union[float, int]=2, baseline: int=True):
    """
    One single run with a possible nonstationary variant of 10-armed bandit
    problem with time steps = horizon.
    
    For action-value methods:
    - sample-averages or exponential recency-weighted average for action-value estimates
    - epsilon-greedy or UCB1 for action selection methods
    
    For gradient bandit:
    - with or without reward baseline. If with reward baseline, it is computed
    by sample-averages or exp. recency-weighted avg. mostly for nonstationary
    problems
    
    Can choose to change initial estimates or not (optimistic initial estimates
    for example)

    Parameters
    ----------
    env : MultiArmedBanditEnv
        k-armed bandit environment (nonassociative). Ex: NonStationaryTestBedEnv
    horizon : int
        Maximum time steps for the run.
    loc : float, optional
        Specify that action values are generated according to a unit-variance
        normal distribution centered around loc (for the environment).
        The default is 0.
    estimation_method : str, optional
        Method used to estimate action values. The default is "sample-avg".
        For constant step size parameter, use "exponential recency-weighted avg"
        and set alpha parameter
    alpha : float
        Constant step size parameter. The default is 0.1. Can be used for
        estimation_method or for gradient bandit.
    Q_init : float, int or np.ndarray
        Initial estimate of the action values, Q_init will be broadcasted to
        the shape (number of actions, ). The default is 0.
    action_selection_method : str, optional
        Method used to select actions. The default is "eps-greedy".
        Other available methods: "ucb", "gradient bandit"
    epsilon : float
        For "eps-greedy" : Probability of taking a random action from all possible actions.
        1-epsilon is the probability to take a greedy action.
    c : float or int
        Degree of exploration for UCB. The default is 2.
    baseline : int
        Baseline for gradient bandit algorithm.
        
        If 1, the reward baseline will be the empirical mean of rewards
        received including current time step (in the book it was included
        for numerical experiments but not in the theory).
        If 2 or upper, the baseline is based on an exponential recency weighted
        average using alpha parameter.
        
        Otherwise the baseline is 0. The default is True.
        
    Returns
    -------
    np.ndarray
        Actual action-values.
    list_Q : list
        ndarrays containing estimates of action values for each time step.
    list_reward : list
        rewards for each time step.
    p_opt : ndarray of shape (horizon, )
        ndarray containing 1's and 0's with 1's at time steps where we took
        an optimal action.

    """
    
    # Estimated action values
    if isinstance(Q_init, float) or isinstance(Q_init, int) or isinstance(Q_init, np.ndarray):
        Q = np.ones(env.action_space.shape, dtype=np.float64)*Q_init
    else:
        raise Exception("Q_init cannot be broadcasted to get initial action-values estimates or is not a float, not an int and not numpy array")
    
    # Number of times action was taken
    N = np.zeros(env.action_space.shape)
    # Array containing 1's and 0's with 1's at time steps where we took
    # an optimal action
    p_opt = np.zeros((horizon,))
    
    # For gradient bandit
    action_preferences = np.zeros(env.action_space.shape)
    with_reward_baseline = baseline
    baseline = 0
    
    list_Q = []
    list_reward = []
    
    
    done = False
    while not done:
        # Select action: strategy or policy
        action = get_action(action_selection_method=action_selection_method, epsilon=epsilon, Q=Q, N=N, c=c, t=env.t, action_preferences=action_preferences)
        
        reward, done = env.step(action)
        
        # Increment count
        N[action] += 1
        # If action is an optimal action (can have multiple)
        # Optimal action can change over time !!
        if action in argmax_indices(env.q_star):
            p_opt[env.t - 1] += 1  # t-1 because t was incremented
            
        # Update estimate of action values or update action preferences
        if action_selection_method != "gradient bandit":
            if estimation_method == "sample-avg":
                Q[action] += 1/N[action]*(reward - Q[action])
            elif estimation_method == "exponential recency-weighted avg":
                Q[action] += alpha*(reward - Q[action])
            else:
                raise Exception("Estimation method does not exist.")
        else:
            # Update estimate of reward baseline
            if reward_baseline_type == 1:
                baseline += with_reward_baseline*(reward - baseline)/env.t
            elif reward_baseline_type >= 2:
                baseline += with_reward_baseline*(reward - baseline)/alpha
            
            # Update action preferences
            action_preferences += alpha*(reward - baseline)*(np.eye(action_preferences.size)[action] - softmax(action_preferences))
        
        # Record Q[action], reward: used for plotting
        list_Q += [Q[action]]
        list_reward += [reward]
        
    env.reset()
    return list_Q, list_reward, p_opt
    