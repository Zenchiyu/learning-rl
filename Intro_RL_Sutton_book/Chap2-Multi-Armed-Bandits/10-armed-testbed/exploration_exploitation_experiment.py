# -*- coding: utf-8 -*-
"""
@author: Stephane Liem Nguyen

In this file we try to replicate experiments from Sutton and Barto's book
for 10 armed testbed
"""
from environment.TenArmedTestBedEnv import *
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

def eps_greedy(epsilon: float, Q: np.ndarray):
    """
    Greedy action selection with respect to Q 
    (one single state, situation. Non associative setting)

    Parameters
    ----------
    epsilon : float
        Probability of taking a random action from all possible actions.
        1-epsilon is the probability to take a greedy action.
    Q : np.ndarray
        Table containing current estimates of action values.

    Returns
    -------
    int
        Action selected by epsilon-greedy.

    """
    if np.random.uniform() < epsilon:
        # random action
        return np.random.choice(np.arange(Q.size))
    else:
        # greedy action
        return np.argmax(Q)
    
def single_run(epsilon: float, horizon: int):
    """
    One single run with a 10-armed bandit problem with time steps = horizon.

    Parameters
    ----------
    epsilon : float
        Probability of taking a random action from all possible actions.
        1-epsilon is the probability to take a greedy action.
    horizon : int
        Maximum time steps for the run.

    Returns
    -------
    np.ndarray
        Actual action-values.
    list_Q : list
        ndarrays containing estimates of action values for each time step.
    list_reward : list
        rewards for each time step.
    list_N_opt : list
        number of times optimal action was taken for each time step.

    """
    env = TenArmedTestBedEnv(horizon=horizon)
    
    # Estimated action values
    Q = np.zeros(env.action_space.shape)
    # Number of times action was taken
    N = np.zeros(env.action_space.shape)
    # Number of times optimal action was taken
    N_opt = 0
    # The optimal action
    opt_action = np.argmax(env.q_star)
    
    list_Q = []
    list_reward = []
    list_N_opt = []
    
    
    done = False
    while not done:
        action = eps_greedy(epsilon, Q)
        reward, done = env.step(action)
        
        # Increment count
        N[action] += 1
        if action == opt_action:
            N_opt += 1
        
        # Update estimate of action values
        Q[action] += 1/N[action]*(reward - Q[action])
        
        # Record Q[action], reward, N_opt : used for plotting
        list_Q += [Q[action]]
        list_reward += [reward]
        list_N_opt += [N_opt]
        
    env.reset()
    return env.q_star, list_Q, list_reward, list_N_opt
    
if __name__ == "__main__":
    np.random.seed(42)  # for reproducibility
    horizon = 1000
    n_runs = 2000
    
    # Greedy : epsilon = 0
    q_starsGreedy, QsGreedy, rewardsGreedy, N_optsGreedy = zip(*[single_run(epsilon=0, horizon=horizon) for _ in range(n_runs)])
    # Eps-Greedy : epsilon = 0.01
    q_stars0dot01, Qs0dot01, rewards0dot01, N_opts0dot01 = zip(*[single_run(epsilon=0.01, horizon=horizon) for _ in range(n_runs)])
    # Eps-Greedy : epsilon = 0.1
    q_stars0dot1, Qs0dot1, rewards0dot1, N_opts0dot1 = zip(*[single_run(epsilon=0.1, horizon=horizon) for _ in range(n_runs)])
    
    # Changing tuples of tuples into arrays
    arr_rewardsGreedy = np.array(rewardsGreedy)
    arr_rewards0dot01 = np.array(rewards0dot01)
    arr_rewards0dot1 = np.array(rewards0dot1)
    
    arr_N_optsGreedy = np.array(N_optsGreedy)
    arr_N_opts0dot01 = np.array(N_opts0dot01)
    arr_N_opts0dot1 = np.array(N_opts0dot1)
    
    # Plots (takes a long time with lineplot)
    # Average Reward
    plt.figure(figsize=(20, 10))
    
    # Get array containing 0 1 2 3 ... 999 repeated 2000 times
    xs = np.broadcast_to(np.arange(horizon), arr_rewardsGreedy.shape).flatten()
    # https://seaborn.pydata.org/generated/seaborn.lineplot.html
    # https://www.reddit.com/r/reinforcementlearning/comments/gnvlcp/way_to_plot_goodlooking_rewards_plots/?utm_source=amp&utm_medium=&utm_content=post_body
    sns.lineplot(x=xs, y=arr_rewardsGreedy.flatten())
    sns.lineplot(x=xs, y=arr_rewards0dot01.flatten())
    sns.lineplot(x=xs, y=arr_rewards0dot1.flatten())
    
    plt.legend([r"$\epsilon=0$", r"$\epsilon=0.01$", r"$\epsilon=0.1$"])
    plt.ylabel("Average reward")
    plt.xlabel("Steps")
    
    
    # Optimal action
    plt.figure(figsize=(20, 10))
    # Get array containing 0 1 2 3 ... 999 repeated 2000 times
    xs = np.broadcast_to(np.arange(horizon), arr_N_optsGreedy.shape).flatten()
    
    # Divide by 1 2 3 ... 1000 elementwise to get the percentage correctly
    sns.lineplot(x=xs, y=arr_N_optsGreedy.flatten()/(xs+1))
    sns.lineplot(x=xs, y=arr_N_opts0dot01.flatten()/(xs+1))
    sns.lineplot(x=xs, y=arr_N_opts0dot1.flatten()/(xs+1))
    plt.legend([r"$\epsilon=0$", r"$\epsilon=0.01$", r"$\epsilon=0.1$"])
    plt.ylabel("% Optimal Action")
    plt.xlabel("Steps")
    