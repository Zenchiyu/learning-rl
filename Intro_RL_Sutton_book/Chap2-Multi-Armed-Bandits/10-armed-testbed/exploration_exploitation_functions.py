# -*- coding: utf-8 -*-
"""
@author: Stephane Liem Nguyen

In this file we write functions used for replicating experiments
from Sutton and Barto's book for 10 armed testbed
"""
from typing import Union
from environment.TenArmedTestBedEnv import *
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

def argmax_indices(Q: np.ndarray):
    """
    Return a numpy array containing all indices of the occurrences of the max.
    np.argmax only gives the first occurrence.

    Parameters
    ----------
    Q : np.ndarray
        Table containing current estimates of action values.

    Returns
    -------
    ndarray of dtype int64
        all indices of the occurrences of the max.

    """
    return np.argwhere(Q == np.max(Q)).flatten()

def eps_greedy(epsilon: float, Q: np.ndarray):
    """
    Epsilon-Greedy action selection with respect to Q 
    (one single state, situation. Non associative setting)

    Parameters
    ----------
    epsilon : float
        Probability of taking a random action from all possible actions.
        1-epsilon is the probability to take a greedy action.
    Q : np.ndarray
        Table containing current estimates of action values.

    Returns
    -------
    int
        Action selected by epsilon-greedy.

    """
    if np.random.uniform() < epsilon:
        # random action
        return np.random.choice(np.arange(Q.size))
    else:
        # greedy action with ties broken randomly
        return np.random.choice(argmax_indices(Q))

def ucb(Q: np.ndarray, N: np.ndarray, c: Union[int, float], t: int):
    """
    Upper-Confidence-Bound (UCB1) from p. 35 of Sutton and Barto's book.
    
    Parameters
    ----------
    Q : np.ndarray
        Table containing current estimates of action values.
    N : np.ndarray
        Table containing number of times the actions were taken.
    c : Union[int, float]
        Degree of exploration.
    t : int
        Current time step (we add 1 because everything is shifted by 1,
                           because t starts at 0 instead of 1)

    Returns
    -------
    int
        Action selected by UCB1.

    """
    assert Q.shape == N.shape, "Q should be of same shape as N"
    # We add 1 to t because t starts at 0 instead of 1
    U = c*np.sqrt(np.divide(np.log(t + 1), N, out=np.zeros(N.shape), where=N!=0))
    
    # All actions with N(a) = 0 are also considered to be maximizing actions
    maximizing_actions = np.argwhere(N == 0).flatten()
    
    maximizing_actions = np.unique(np.append(maximizing_actions, argmax_indices(Q + U)))
    return np.random.choice(maximizing_actions)

def softmax(action_preferences):
    return np.exp(action_preferences)/np.sum(np.exp(action_preferences))

def gradient_bandit(action_preferences: np.ndarray):
    """
    Action selection based on a soft-max distribution using action preferences.

    Parameters
    ----------
    action_preferences : ndarray

    Returns
    -------
    int
        Action selected based on a soft-max distribution using action
        preferences.

    """
    return np.random.choice(np.arange(action_preferences.size), p=softmax(action_preferences))

def get_action(action_selection_method: str="eps-greedy", **kwargs):
    if action_selection_method == "eps-greedy":
        action = eps_greedy(kwargs["epsilon"], kwargs["Q"])
    elif action_selection_method == "ucb":
        action = ucb(kwargs["Q"], kwargs["N"], kwargs["c"], kwargs["t"])  # env.t
    elif action_selection_method == "gradient bandit":
        action = gradient_bandit(kwargs["action_preferences"])
    else:
        raise Exception("Action selection method (strategy, policy) does not exist.")
    return action
            
def single_run(horizon: int, loc=0, estimation_method: str="sample-avg", alpha: float=0.1, Q_init: Union[float, int, np.ndarray]=0, action_selection_method: str="eps-greedy", epsilon: float=0.1, c: Union[float, int]=2, baseline: bool=True):
    """
    One single run with a 10-armed bandit problem with time steps = horizon.

    Parameters
    ----------
    horizon : int
        Maximum time steps for the run.
    loc : float, optional
        Specify that action values are generated according to a unit-variance
        normal distribution centered around loc (for the environment).
        The default is 0.
    estimation_method : str, optional
        Method used to estimate action values. The default is "sample-avg".
        For constant step size parameter, use "exponential recency-weighted avg"
        and set alpha parameter
    alpha : float
        Constant step size parameter. The default is 0.1. Can be used for
        estimation_method or for gradient bandit.
    Q_init : float, int or np.ndarray
        Initial estimate of the action values, Q_init will be broadcasted to
        the shape (number of actions, ). The default is 0.
    action_selection_method : str, optional
        Method used to select actions. The default is "eps-greedy".
        Other available methods: "ucb"
    epsilon : float
        For "eps-greedy" : Probability of taking a random action from all possible actions.
        1-epsilon is the probability to take a greedy action.
    c : float or int
        Degree of exploration for UCB. The default is 2.
    baseline : bool
        Baseline for gradient bandit algorithm.
        
        If true, the reward baseline will be the empirical mean of rewards
        received including current time step (in the book it was included
        for numerical experiments but not in the theory).
        Otherwise the baseline is 0. The default is True.
        
    Returns
    -------
    np.ndarray
        Actual action-values.
    list_Q : list
        ndarrays containing estimates of action values for each time step.
    list_reward : list
        rewards for each time step.
    p_opt : ndarray of shape (horizon, )
        ndarray containing 1's and 0's with 1's at time steps where we took
        an optimal action.

    """
    env = TenArmedTestBedEnv(horizon=horizon, loc=loc)
    
    # Estimated action values
    if isinstance(Q_init, float) or isinstance(Q_init, int) or isinstance(Q_init, np.ndarray):
        Q = np.ones(env.action_space.shape, dtype=np.float64)*Q_init
    else:
        raise Exception("Q_init cannot be broadcasted to get initial action-values estimates or is not a float, not an int and not numpy array")
    
    # Number of times action was taken
    N = np.zeros(env.action_space.shape)
    # The optimal actions (do not change over time)
    opt_actions = argmax_indices(env.q_star)
    # Array containing 1's and 0's with 1's at time steps where we took
    # an optimal action
    p_opt = np.zeros((horizon,))
    
    # For gradient bandit
    action_preferences = np.zeros(env.action_space.shape)
    with_reward_baseline = baseline
    baseline = 0
    
    list_Q = []
    list_reward = []
    
    
    done = False
    while not done:
        # Select action: strategy or policy
        action = get_action(action_selection_method=action_selection_method, epsilon=epsilon, Q=Q, N=N, c=c, t=env.t, action_preferences=action_preferences)
        
        reward, done = env.step(action)
        
        # Increment count
        N[action] += 1
        # If action is an optimal action (can have multiple)
        if action in opt_actions:
            p_opt[env.t - 1] += 1  # t-1 because t was incremented
            
        # Update estimate of action values or update action preferences
        if action_selection_method != "gradient bandit":
            if estimation_method == "sample-avg":
                Q[action] += 1/N[action]*(reward - Q[action])
            elif estimation_method == "exponential recency-weighted avg":
                Q[action] += alpha*(reward - Q[action])
            else:
                raise Exception("Estimation method does not exist.")
        else:
            # Update estimate of reward baseline
            baseline += with_reward_baseline*(reward - baseline)/env.t
            # Update action preferences
            action_preferences += alpha*(reward - baseline)*(np.eye(action_preferences.size)[action] - softmax(action_preferences))
        
        # Record Q[action], reward: used for plotting
        list_Q += [Q[action]]
        list_reward += [reward]
        
    env.reset()
    return env.q_star, list_Q, list_reward, p_opt
    
    