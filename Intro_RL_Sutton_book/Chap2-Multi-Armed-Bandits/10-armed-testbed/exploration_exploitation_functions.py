# -*- coding: utf-8 -*-
"""
@author: Stephane Liem Nguyen

In this file we write functions used for replicating experiments
from Sutton and Barto's book for 10 armed testbed
"""
from typing import Union
from environment.TenArmedTestBedEnv import *
import matplotlib.pyplot as plt
import seaborn as sns
sns.set()

def argmax_indices(Q: np.ndarray):
    """
    Return a numpy array containing all indices of the occurrences of the max.
    np.argmax only gives the first occurrence.

    Parameters
    ----------
    Q : np.ndarray
        Table containing current estimates of action values.

    Returns
    -------
    ndarray of dtype int64
        all indices of the occurrences of the max.

    """
    return np.argwhere(Q == np.max(Q)).flatten()

def eps_greedy(epsilon: float, Q: np.ndarray):
    """
    Epsilon-Greedy action selection with respect to Q 
    (one single state, situation. Non associative setting)

    Parameters
    ----------
    epsilon : float
        Probability of taking a random action from all possible actions.
        1-epsilon is the probability to take a greedy action.
    Q : np.ndarray
        Table containing current estimates of action values.

    Returns
    -------
    int
        Action selected by epsilon-greedy.

    """
    if np.random.uniform() < epsilon:
        # random action
        return np.random.choice(np.arange(Q.size))
    else:
        # greedy action with ties broken randomly
        return np.random.choice(argmax_indices(Q))

def ucb(Q: np.ndarray, N: np.ndarray, c: Union[int, float], t: int):
    """
    Upper-Confidence-Bound (UCB1) from p. 35 of Sutton and Barto's book.
    
    Parameters
    ----------
    Q : np.ndarray
        Table containing current estimates of action values.
    N : np.ndarray
        Table containing number of times the actions were taken.
    c : Union[int, float]
        Degree of exploration.
    t : int
        Current time step (we add 1 because everything is shifted by 1,
                           because t starts at 0 instead of 1)

    Returns
    -------
    int
        Action selected by UCB1.

    """
    assert Q.shape == N.shape, "Q should be of same shape as N"
    # We add 1 to t because t starts at 0 instead of 1
    U = c*np.sqrt(np.divide(np.log(t + 1), N, out=np.zeros(N.shape), where=N!=0))
    
    # All actions with N(a) = 0 are also considered to be maximizing actions
    maximizing_actions = np.argwhere(N == 0).flatten()
    
    maximizing_actions = np.unique(np.append(maximizing_actions, argmax_indices(Q + U)))
    return np.random.choice(maximizing_actions)
    
def single_run(epsilon: float, horizon: int, estimation_method: str="sample-avg", alpha: float=0.1, Q_init: Union[float, int, np.ndarray]=0, action_selection_method: str="eps-greedy", c: Union[float, int]=2):
    """
    One single run with a 10-armed bandit problem with time steps = horizon.

    Parameters
    ----------
    epsilon : float
        Probability of taking a random action from all possible actions.
        1-epsilon is the probability to take a greedy action.
    horizon : int
        Maximum time steps for the run.
    estimation_method : str, optional
        Method used to estimate action values. The default is "sample-avg".
        For constant step size parameter, use "exponential recency-weighted avg"
        and set alpha parameter
    alpha : float
        Constant step size parameter. The default is 0.1.
    Q_init : float, int or np.ndarray
        Initial estimate of the action values, Q_init will be broadcasted to
        the shape (number of actions, ). The default is 0.
    action_selection_method : str, optional
        Method used to select actions. The default is "eps-greedy".
        Other available methods: "ucb"
    c : float or int
        Degree of exploration. The default is 2
        
    Returns
    -------
    np.ndarray
        Actual action-values.
    list_Q : list
        ndarrays containing estimates of action values for each time step.
    list_reward : list
        rewards for each time step.
    p_opt : ndarray of shape (horizon, )
        ndarray containing 1's and 0's with 1's at time steps where we took
        an optimal action.

    """
    env = TenArmedTestBedEnv(horizon=horizon)
    
    # Estimated action values
    if isinstance(Q_init, float) or isinstance(Q_init, int) or isinstance(Q_init, np.ndarray):
        Q = np.ones(env.action_space.shape, dtype=np.float64)*Q_init
    else:
        raise Exception("Q_init cannot be broadcasted to get initial action-values estimates or is not a float, not an int and not numpy array")
    
    # Number of times action was taken
    N = np.zeros(env.action_space.shape)
    # The optimal actions (do not change over time)
    opt_actions = argmax_indices(env.q_star)
    # Array containing 1's and 0's with 1's at time steps where we took
    # an optimal action
    p_opt = np.zeros((horizon,))
    
    list_Q = []
    list_reward = []
    
    
    done = False
    while not done:
        # Select action: strategy or policy
        if action_selection_method == "eps-greedy":
            action = eps_greedy(epsilon, Q)
        elif action_selection_method == "ucb":
            action = ucb(Q, N, c, env.t)
        else:
            raise Exception("Action selection method (strategy, policy) does not exist.")
            
        reward, done = env.step(action)
        
        # Increment count
        N[action] += 1
        # If action is an optimal action (can have multiple)
        if action in opt_actions:
            p_opt[env.t - 1] += 1  # t-1 because t was incremented
            
        # Update estimate of action values
        if estimation_method == "sample-avg":
            Q[action] += 1/N[action]*(reward - Q[action])
        elif estimation_method == "exponential recency-weighted avg":
            Q[action] += alpha*(reward - Q[action])
        else:
            raise Exception("Estimation method does not exist.")
        
        # Record Q[action], reward: used for plotting
        list_Q += [Q[action]]
        list_reward += [reward]
        
    env.reset()
    return env.q_star, list_Q, list_reward, p_opt
    
    