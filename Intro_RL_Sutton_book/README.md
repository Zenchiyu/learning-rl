# [Reinforcement Learning : An Introduction (second edition)](http://incompleteideas.net/book/RLbook2020.pdf)

We'll write "Replica" for the codes where we tried to replicate some figures. They can differ from the book. We only link to programming exercises below. Other exercises will be in pdf format.

My work is not necessarily free of mistakes. Please contact me if you see possible improvements so I can update the files.

## Chapter 1 - Introduction

### Replica
### Exercises
### Notes

# Tabular Solution Methods
## Chapter 2 - Multi-armed Bandits

### Replica
* [Figure 2.2: Average performance of epsilon-greedy action-value methods on the 10-armed testbed](https://github.com/Zenchiyu/learning-rl/tree/develop/Intro_RL_Sutton_book/Chap2-Multi-Armed-Bandits/10-armed-testbed#effectiveness-of-greedy-or-epsilon-greedy-action-value-methods)
* [Figure 2.3: The effect of optimistic initial action-value estimates on the 10-armed testbed](https://github.com/Zenchiyu/learning-rl/tree/develop/Intro_RL_Sutton_book/Chap2-Multi-Armed-Bandits/10-armed-testbed#optimistic-initial-values)
* [Figure 2.4: Average performance of UCB action selection on the 10-armed testbed](https://github.com/Zenchiyu/learning-rl/tree/develop/Intro_RL_Sutton_book/Chap2-Multi-Armed-Bandits/10-armed-testbed#upper-confidence-bound)
* (Work In Progress) Figure 2.5: Average performance of the gradient bandit algorithm with and without a reward
baseline on variant of the 10-armed testbed
* (Work In Progress) Figure 2.6: A parameter study of the various bandit algorithms presented in this chapter.

### Exercises
* [Exercise 2.5: Experiment to demonstrate the
difficulties that sample-average methods have for nonstationary problems](https://github.com/Zenchiyu/learning-rl/tree/develop/Intro_RL_Sutton_book/Chap2-Multi-Armed-Bandits/non-stationary-testbed#non-stationary-testbed)
* (Work In Progress) Exercice 2.11: Parameter Study for nonstationary case

### Notes

## Chapter 3 - Finite Markov Decision Processes (Finite MDP)
### Replica
### Exercises
### Notes

## Chapter 4 - Dynamic Programming (DP)
### Replica
### Exercises
### Notes

## Chapter 5 - Monte-Carlo Methods (MC learning)
### Replica
### Exercises
### Notes

## Chapter 6 - Temporal-Difference Learning (TD learning)
### Replica
### Exercises
### Notes

## Chapter 7 - n-step Bootstrapping (multi-step Boostrapping)
### Replica
### Exercises
### Notes

## Chapter 8 - Planning and Learning with Tabular Methods
### Replica
### Exercises
### Notes

We'll add the rest later.
