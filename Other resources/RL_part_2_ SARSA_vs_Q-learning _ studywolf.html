<!DOCTYPE html>
<!-- saved from url=(0086)https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/ -->
<html lang="en"><link type="text/css" rel="stylesheet" id="dark-mode-custom-link"><link type="text/css" rel="stylesheet" id="dark-mode-general-link"><style lang="en" type="text/css" id="dark-mode-custom-style"></style><style lang="en" type="text/css" id="dark-mode-native-style"></style><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	
	<title>Reinforcement Learning part 2: SARSA vs Q-learning | studywolf</title>
	<link rel="profile" href="http://gmpg.org/xfn/11">
	<link rel="pingback" href="https://studywolf.wordpress.com/xmlrpc.php">
	<meta name="robots" content="max-image-preview:large">
<link rel="dns-prefetch" href="https://s0.wp.com/">
<link rel="dns-prefetch" href="https://fonts.googleapis.com/">
<link rel="dns-prefetch" href="https://s.pubmine.com/">
<link rel="dns-prefetch" href="https://x.bidswitch.net/">
<link rel="dns-prefetch" href="https://static.criteo.net/">
<link rel="dns-prefetch" href="https://ib.adnxs.com/">
<link rel="dns-prefetch" href="https://aax.amazon-adsystem.com/">
<link rel="dns-prefetch" href="https://bidder.criteo.com/">
<link rel="dns-prefetch" href="https://cas.criteo.com/">
<link rel="dns-prefetch" href="https://gum.criteo.com/">
<link rel="dns-prefetch" href="https://ads.pubmatic.com/">
<link rel="dns-prefetch" href="https://gads.pubmatic.com/">
<link rel="dns-prefetch" href="https://tpc.googlesyndication.com/">
<link rel="dns-prefetch" href="https://ad.doubleclick.net/">
<link rel="dns-prefetch" href="https://googleads.g.doubleclick.net/">
<link rel="dns-prefetch" href="https://www.googletagservices.com/">
<link rel="dns-prefetch" href="https://cdn.switchadhub.com/">
<link rel="dns-prefetch" href="https://delivery.g.switchadhub.com/">
<link rel="dns-prefetch" href="https://delivery.swid.switchadhub.com/">
<link rel="dns-prefetch" href="https://a.teads.tv/">
<link rel="dns-prefetch" href="https://prebid.media.net/">
<link rel="dns-prefetch" href="https://adserver-us.adtech.advertising.com/">
<link rel="dns-prefetch" href="https://fastlane.rubiconproject.com/">
<link rel="dns-prefetch" href="https://prebid-server.rubiconproject.com/">
<link rel="dns-prefetch" href="https://hb-api.omnitagjs.com/">
<link rel="dns-prefetch" href="https://mtrx.go.sonobi.com/">
<link rel="dns-prefetch" href="https://apex.go.sonobi.com/">
<link rel="dns-prefetch" href="https://u.openx.net/">
<link rel="alternate" type="application/rss+xml" title="studywolf » Feed" href="https://studywolf.wordpress.com/feed/">
<link rel="alternate" type="application/rss+xml" title="studywolf » Comments Feed" href="https://studywolf.wordpress.com/comments/feed/">
<link rel="alternate" type="application/rss+xml" title="studywolf » Reinforcement Learning part 2: SARSA vs Q-learning Comments Feed" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/feed/">
	<script type="text/javascript">
		/* <![CDATA[ */
		function addLoadEvent(func) {
			var oldonload = window.onload;
			if (typeof window.onload != 'function') {
				window.onload = func;
			} else {
				window.onload = function () {
					oldonload();
					func();
				}
			}
		}
		/* ]]> */
	</script>
			<script type="text/javascript">
			window._wpemojiSettings = {"baseUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/72x72\/","ext":".png","svgUrl":"https:\/\/s0.wp.com\/wp-content\/mu-plugins\/wpcom-smileys\/twemoji\/2\/svg\/","svgExt":".svg","source":{"concatemoji":"https:\/\/s0.wp.com\/wp-includes\/js\/wp-emoji-release.min.js?m=1625065786h&ver=5.8.1-alpha-51081"}};
			!function(e,a,t){var n,r,o,i=a.createElement("canvas"),p=i.getContext&&i.getContext("2d");function s(e,t){var a=String.fromCharCode;p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,e),0,0);e=i.toDataURL();return p.clearRect(0,0,i.width,i.height),p.fillText(a.apply(this,t),0,0),e===i.toDataURL()}function c(e){var t=a.createElement("script");t.src=e,t.defer=t.type="text/javascript",a.getElementsByTagName("head")[0].appendChild(t)}for(o=Array("flag","emoji"),t.supports={everything:!0,everythingExceptFlag:!0},r=0;r<o.length;r++)t.supports[o[r]]=function(e){if(!p||!p.fillText)return!1;switch(p.textBaseline="top",p.font="600 32px Arial",e){case"flag":return s([127987,65039,8205,9895,65039],[127987,65039,8203,9895,65039])?!1:!s([55356,56826,55356,56819],[55356,56826,8203,55356,56819])&&!s([55356,57332,56128,56423,56128,56418,56128,56421,56128,56430,56128,56423,56128,56447],[55356,57332,8203,56128,56423,8203,56128,56418,8203,56128,56421,8203,56128,56430,8203,56128,56423,8203,56128,56447]);case"emoji":return!s([10084,65039,8205,55357,56613],[10084,65039,8203,55357,56613])}return!1}(o[r]),t.supports.everything=t.supports.everything&&t.supports[o[r]],"flag"!==o[r]&&(t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&t.supports[o[r]]);t.supports.everythingExceptFlag=t.supports.everythingExceptFlag&&!t.supports.flag,t.DOMReady=!1,t.readyCallback=function(){t.DOMReady=!0},t.supports.everything||(n=function(){t.readyCallback()},a.addEventListener?(a.addEventListener("DOMContentLoaded",n,!1),e.addEventListener("load",n,!1)):(e.attachEvent("onload",n),a.attachEvent("onreadystatechange",function(){"complete"===a.readyState&&t.readyCallback()})),(n=t.source||{}).concatemoji?c(n.concatemoji):n.wpemoji&&n.twemoji&&(c(n.twemoji),c(n.wpemoji)))}(window,document,window._wpemojiSettings);
		</script><script src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/wp-emoji-release.min.js.téléchargé" type="text/javascript" defer=""></script>
		<style type="text/css">
img.wp-smiley,
img.emoji {
	display: inline !important;
	border: none !important;
	box-shadow: none !important;
	height: 1em !important;
	width: 1em !important;
	margin: 0 .07em !important;
	vertical-align: -0.1em !important;
	background: none !important;
	padding: 0 !important;
}
</style>
	<link rel="stylesheet" id="all-css-0-1" href="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/saved_resource(2)" type="text/css" media="all">
<style id="wp-block-library-inline-css">
.has-text-align-justify {
	text-align:justify;
}
</style>
<link crossorigin="anonymous" rel="stylesheet" id="oswald-css" href="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/css" media="all">
<link rel="stylesheet" id="all-css-2-1" href="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/saved_resource(3)" type="text/css" media="all">
<link rel="stylesheet" id="print-css-3-1" href="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/global-print.css" type="text/css" media="print">
<style id="jetpack-global-styles-frontend-style-inline-css">
:root { --font-headings: unset; --font-base: unset; --font-headings-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif; --font-base-default: -apple-system,BlinkMacSystemFont,"Segoe UI",Roboto,Oxygen-Sans,Ubuntu,Cantarell,"Helvetica Neue",sans-serif;}
</style>
<link rel="stylesheet" id="all-css-6-1" href="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/saved_resource(4)" type="text/css" media="all">
<script id="jetpack_related-posts-js-extra">
var related_posts_js_options = {"post_heading":"h4"};
</script>
<script id="wpcom-actionbar-placeholder-js-extra">
var actionbardata = {"siteID":"39795907","siteName":"studywolf","siteURL":"http:\/\/studywolf.wordpress.com","siteHost":"studywolf.wordpress.com","icon":"<img alt='' src='https:\/\/s0.wp.com\/i\/logo\/wpcom-gray-white.png' class='avatar avatar-50' height='50' width='50' \/>","canManageOptions":"","canCustomizeSite":"","isFollowing":"","themeSlug":"pub\/chunk","signupURL":"https:\/\/wordpress.com\/start\/","loginURL":"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fstudywolf.wordpress.com%2F2013%2F07%2F01%2Freinforcement-learning-sarsa-vs-q-learning%2F&signup_flow=account","themeURL":"","xhrURL":"https:\/\/studywolf.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"e3dd1bcf5c","isSingular":"1","isFolded":"","isLoggedIn":"","isMobile":"","subscribeNonce":"<input type=\"hidden\" id=\"_wpnonce\" name=\"_wpnonce\" value=\"5864c4970b\" \/>","referer":"https:\/\/studywolf.wordpress.com\/2013\/07\/01\/reinforcement-learning-sarsa-vs-q-learning\/","canFollow":"1","feedID":"4685655","statusMessage":"","customizeLink":"https:\/\/studywolf.wordpress.com\/wp-admin\/customize.php?url=https%3A%2F%2Fstudywolf.wordpress.com%2F2013%2F07%2F01%2Freinforcement-learning-sarsa-vs-q-learning%2F","postID":"1043","shortlink":"https:\/\/wp.me\/p2GYJt-gP","canEditPost":"","editLink":"https:\/\/wordpress.com\/post\/studywolf.wordpress.com\/1043","statsLink":"https:\/\/wordpress.com\/stats\/post\/1043\/studywolf.wordpress.com","i18n":{"view":"View site","follow":"Follow","following":"Following","edit":"Edit","login":"Log in","signup":"Sign up","customize":"Customize","report":"Report this content","themeInfo":"Get theme: Chunk","shortlink":"Copy shortlink","copied":"Copied","followedText":"New posts from this site will now appear in your <a href=\"https:\/\/wordpress.com\/read\">Reader<\/a>","foldBar":"Collapse this bar","unfoldBar":"Expand this bar","editSubs":"Manage subscriptions","viewReader":"View site in Reader","viewReadPost":"View post in Reader","subscribe":"Sign me up","enterEmail":"Enter your email address","followers":"Join 258 other followers","alreadyUser":"Already have a WordPress.com account? <a href=\"https:\/\/wordpress.com\/log-in?redirect_to=https%3A%2F%2Fstudywolf.wordpress.com%2F2013%2F07%2F01%2Freinforcement-learning-sarsa-vs-q-learning%2F&signup_flow=account\">Log in now.<\/a>","stats":"Stats"}};
</script>
<script crossorigin="anonymous" type="text/javascript" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/saved_resource(5)"></script>
<script type="text/javascript">
	window.addEventListener( 'DOMContentLoaded', function() {
		rltInitialize( {"token":null,"iframeOrigins":["https:\/\/widgets.wp.com"]} );
	} );
</script>
<link rel="stylesheet" id="all-css-0-2" href="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/style(1).css" type="text/css" media="all">
<!--[if lt IE 8]>
<link rel='stylesheet' id='highlander-comments-ie7-css'  href='https://s0.wp.com/wp-content/mu-plugins/highlander-comments/style-ie7.css?m=1351637563h&#038;ver=20110606' media='all' />
<![endif]-->
<link rel="EditURI" type="application/rsd+xml" title="RSD" href="https://studywolf.wordpress.com/xmlrpc.php?rsd">
<link rel="wlwmanifest" type="application/wlwmanifest+xml" href="https://s0.wp.com/wp-includes/wlwmanifest.xml"> 
<meta name="generator" content="WordPress.com">
<link rel="canonical" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/">
<link rel="shortlink" href="https://wp.me/p2GYJt-gP">
<link rel="alternate" type="application/json+oembed" href="https://public-api.wordpress.com/oembed/?format=json&amp;url=https%3A%2F%2Fstudywolf.wordpress.com%2F2013%2F07%2F01%2Freinforcement-learning-sarsa-vs-q-learning%2F&amp;for=wpcom-auto-discovery"><link rel="alternate" type="application/xml+oembed" href="https://public-api.wordpress.com/oembed/?format=xml&amp;url=https%3A%2F%2Fstudywolf.wordpress.com%2F2013%2F07%2F01%2Freinforcement-learning-sarsa-vs-q-learning%2F&amp;for=wpcom-auto-discovery">
<!-- Jetpack Open Graph Tags -->
<meta property="og:type" content="article">
<meta property="og:title" content="Reinforcement Learning part 2: SARSA vs Q-learning">
<meta property="og:url" content="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/">
<meta property="og:description" content="In my previous post about reinforcement learning I talked about Q-learning, and how that works in the context of a cat vs mouse game. I mentioned in this post that there are a number of other metho…">
<meta property="article:published_time" content="2013-07-02T02:50:29+00:00">
<meta property="article:modified_time" content="2013-09-17T23:40:38+00:00">
<meta property="og:site_name" content="studywolf">
<meta property="og:image" content="https://studywolf.files.wordpress.com/2013/06/mouse-vs-cliff.png?w=300">
<meta property="og:image:width" content="300">
<meta property="og:image:height" content="131">
<meta property="og:image:alt" content="mouse vs cliff">
<meta property="og:locale" content="en_US">
<meta name="twitter:site" content="@wordpressdotcom">
<meta name="twitter:text:title" content="Reinforcement Learning part 2: SARSA vs Q-learning">
<meta name="twitter:image" content="https://studywolf.files.wordpress.com/2013/06/mouse-vs-cliff.png?w=144">
<meta name="twitter:image:alt" content="mouse vs cliff">
<meta name="twitter:card" content="summary">
<meta property="fb:app_id" content="249643311490">
<meta property="article:publisher" content="https://www.facebook.com/WordPresscom">

<!-- End Jetpack Open Graph Tags -->
<link rel="shortcut icon" type="image/x-icon" href="https://s0.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48">
<link rel="icon" type="image/x-icon" href="https://s0.wp.com/i/favicon.ico" sizes="16x16 24x24 32x32 48x48">
<link rel="apple-touch-icon" href="https://s0.wp.com/i/webclip.png">
<link rel="openid.server" href="https://studywolf.wordpress.com/?openidserver=1">
<link rel="openid.delegate" href="https://studywolf.wordpress.com/">
<link rel="search" type="application/opensearchdescription+xml" href="https://studywolf.wordpress.com/osd.xml" title="studywolf">
<link rel="search" type="application/opensearchdescription+xml" href="https://s1.wp.com/opensearch.xml" title="WordPress.com">
<meta name="application-name" content="studywolf"><meta name="msapplication-window" content="width=device-width;height=device-height"><meta name="msapplication-tooltip" content="a blog for things I encounter while coding and researching neuroscience, motor control, and learning"><meta name="msapplication-task" content="name=Subscribe;action-uri=https://studywolf.wordpress.com/feed/;icon-uri=https://s0.wp.com/i/favicon.ico"><meta name="msapplication-task" content="name=Sign up for a free blog;action-uri=http://wordpress.com/signup/;icon-uri=https://s0.wp.com/i/favicon.ico"><meta name="msapplication-task" content="name=WordPress.com Support;action-uri=http://support.wordpress.com/;icon-uri=https://s0.wp.com/i/favicon.ico"><meta name="msapplication-task" content="name=WordPress.com Forums;action-uri=http://forums.wordpress.com/;icon-uri=https://s0.wp.com/i/favicon.ico"><meta name="description" content="In my previous post about reinforcement learning I talked about Q-learning, and how that works in the context of a cat vs mouse game. I mentioned in this post that there are a number of other methods of reinforcement learning aside from Q-learning, and today I&#39;ll talk about another one of them: SARSA. All the…">
		<script type="text/javascript">

			window.doNotSellCallback = function() {

				var linkElements = [
					'a[href="https://wordpress.com/?ref=footer_blog"]',
					'a[href="https://wordpress.com/?ref=footer_website"]',
					'a[href="https://wordpress.com/?ref=vertical_footer"]',
					'a[href^="https://wordpress.com/?ref=footer_segment_"]',
				].join(',');

				var dnsLink = document.createElement( 'a' );
				dnsLink.href = 'https://wordpress.com/advertising-program-optout/';
				dnsLink.classList.add( 'do-not-sell-link' );
				dnsLink.rel = 'nofollow';
				dnsLink.style.marginLeft = '0.5em';
				dnsLink.textContent = 'Do Not Sell My Personal Information';

				var creditLinks = document.querySelectorAll( linkElements );

				if ( 0 === creditLinks.length ) {
					return false;
				}

				Array.prototype.forEach.call( creditLinks, function( el ) {
					el.insertAdjacentElement( 'afterend', dnsLink );
				});

				return true;
			};

		</script>
		<script id="cmp-configuration" type="application/configuration">{"gvlVersion":"105","consentLanguage":"EN","locale":"en","vendorsAll":"BBdr_6__7a_s_3_fv_9ujzGr_v9f9_yGccL7tv3guxf635ei_-wnZou_VNXBVyPEl27YJCBto5k6iak2LVEqteZ9jUmxlfRpRPZck09jL2zrAw9p8_uqfzJTPf_f__7_e-f___v___v-_____________3_____v____________v____wAA","vendorsLegInterest":"BBcAkw1LyALsyxwZNo0qhRAjCsJDoBQAUUAwtE1gAwOCnZWAR6ghYAITUBGBECDEFGDAIABBIAkIiAkALBAIgCIBAACAFSAhAARMAgsALAwCAAUA0LECKAIQJCDI4KjlMCAqRaKCeysASi72NMIQy3wIoFH9FRgI1miBYGQkLBzHAEgJeAAA","ajaxNonce":"16aa0be648","modulePath":"https:\/\/s0.wp.com\/wp-content\/blog-plugins\/wordads-classes\/js\/","gvlPath":"https:\/\/public-api.wordpress.com\/wpcom\/v2\/sites\/39795907\/cmp\/vendors\/en\/","_":{"title":"Privacy & Cookies","intro":"We, WordPress.com, and our advertising partners store and\/or access information on your device and also process personal data, like unique identifiers, browsing activity, and other standard information sent by your device including your IP address. This information is collected over time and used for personalised ads, ad measurement, audience insights, and product development specific to our ads program. If this sounds good to you, select \"I Agree!\" below.  Otherwise, you can get more information, customize your consent preferences, or decline consent by selecting \"Learn More\". Note that your preferences apply to all websites in the <a href=\"https:\/\/automattic.com\/cookies\/#user-ads-consent\" target=\"_blank\">WordPress.com network<\/a>, and if you change your mind in the future you can update your preferences anytime by visiting the Privacy link displayed under each ad. One last thing, our partners may process some of your data based on legitimate interests instead of consent but you can object to that by choosing \"Learn More\" and then disabling the Legitimate Interests toggle under any listed Purpose or Partner.","config":"Learn More","accept":"I Agree!","viewPartners":"View Partners","error":"We're sorry but an unexpected error occurred. Please try again later."}}</script>		<script type="text/javascript">
		function __ATA_CC() {var v = document.cookie.match('(^|;) ?personalized-ads-consent=([^;]*)(;|$)');return v ? 1 : 0;}
		var __ATA_PP = { 'pt': 1, 'ht': 0, 'tn': 'chunk', 'amp': false, 'consent': __ATA_CC(), 'gdpr_applies': true, 'ad': { 'label': { 'text': 'Advertisements' }, 'reportAd': { 'text': 'Report this ad' }, 'privacySettings': { 'text': 'Privacy', 'onClick': function() { window.__tcfapi && window.__tcfapi( 'showUi' ) } } }, 'siteid': 8982, 'blogid': 39795907, 'js_hint': 'tcf2_test' };
		var __ATA = __ATA || {};
		__ATA.cmd = __ATA.cmd || [];
		__ATA.criteo = __ATA.criteo || {};
		__ATA.criteo.cmd = __ATA.criteo.cmd || [];
		</script>
		<script type="text/javascript">
		(function(){var g=Date.now||function(){return+new Date};function h(a,b){a:{for(var c=a.length,d="string"==typeof a?a.split(""):a,e=0;e<c;e++)if(e in d&&b.call(void 0,d[e],e,a)){b=e;break a}b=-1}return 0>b?null:"string"==typeof a?a.charAt(b):a[b]};function k(a,b,c){c=null!=c?"="+encodeURIComponent(String(c)):"";if(b+=c){c=a.indexOf("#");0>c&&(c=a.length);var d=a.indexOf("?");if(0>d||d>c){d=c;var e=""}else e=a.substring(d+1,c);a=[a.substr(0,d),e,a.substr(c)];c=a[1];a[1]=b?c?c+"&"+b:b:c;a=a[0]+(a[1]?"?"+a[1]:"")+a[2]}return a};var l=0;function m(a,b){var c=document.createElement("script");c.src=a;c.onload=function(){b&&b(void 0)};c.onerror=function(){b&&b("error")};a=document.getElementsByTagName("head");var d;a&&0!==a.length?d=a[0]:d=document.documentElement;d.appendChild(c)}function n(a){var b=void 0===b?document.cookie:b;return(b=h(b.split("; "),function(c){return-1!=c.indexOf(a+"=")}))?b.split("=")[1]:""}function p(a){return"string"==typeof a&&0<a.length}
		function r(a,b,c){b=void 0===b?"":b;c=void 0===c?".":c;var d=[];Object.keys(a).forEach(function(e){var f=a[e],q=typeof f;"object"==q&&null!=f||"function"==q?d.push(r(f,b+e+c)):null!==f&&void 0!==f&&(e=encodeURIComponent(b+e),d.push(e+"="+encodeURIComponent(f)))});return d.filter(p).join("&")}function t(a,b){a||((window.__ATA||{}).config=b.c,m(b.url))}var u=Math.floor(1E13*Math.random()),v=window.__ATA||{};window.__ATA=v;window.__ATA.cmd=v.cmd||[];v.rid=u;v.createdAt=g();var w=window.__ATA||{},x="s.pubmine.com";
		w&&w.serverDomain&&(x=w.serverDomain);var y="//"+x+"/conf",z=window.top===window,A=window.__ATA_PP&&window.__ATA_PP.gdpr_applies,B="boolean"===typeof A?Number(A):null,C=window.__ATA_PP||null,D=z?document.referrer?document.referrer:null:null,E=z?window.location.href:document.referrer?document.referrer:null,F,G=n("__ATA_tuuid");F=G?G:null;var H=window.innerWidth+"x"+window.innerHeight,I=n("usprivacy"),J=r({gdpr:B,pp:C,rid:u,src:D,ref:E,tuuid:F,vp:H,us_privacy:I?I:null},"",".");
		(function(a){var b=void 0===b?"cb":b;l++;var c="callback__"+g().toString(36)+"_"+l.toString(36);a=k(a,b,c);window[c]=function(d){t(void 0,d)};m(a,function(d){d&&t(d)})})(y+"?"+J);}).call(this);
		</script><script src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/conf"></script><link rel="amphtml" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/amp/"><script type="text/javascript">
	window.google_analytics_uacct = "UA-52447-2";
</script>

<script type="text/javascript">
	var _gaq = _gaq || [];
	_gaq.push(['_setAccount', 'UA-52447-2']);
	_gaq.push(['_gat._anonymizeIp']);
	_gaq.push(['_setDomainName', 'wordpress.com']);
	_gaq.push(['_initData']);
	_gaq.push(['_trackPageview']);

	(function() {
		var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
		ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
		(document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(ga);
	})();
</script><script type="text/javascript" async="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/ga.js.téléchargé"></script>
<link rel="stylesheet" type="text/css" href="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/shCore.css"><link rel="stylesheet" type="text/css" href="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/shThemeDefault.css"><link rel="stylesheet" type="text/css" id="gravatar-card-css" href="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/hovercard.min.css"><link rel="stylesheet" type="text/css" id="gravatar-card-services-css" href="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/services.min.css"><link href="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/actionbar.css" type="text/css" rel="stylesheet"></head>
<body class="post-template-default single single-post postid-1043 single-format-standard customizer-styles-applied single-author highlander-enabled highlander-light">

<div id="container">

	<div id="header">
		<h1 id="site-title"><a href="https://studywolf.wordpress.com/" title="studywolf" rel="home">studywolf</a></h1>
		<h2 id="site-description">a blog for things I encounter while coding and researching neuroscience, motor control, and learning</h2>
	</div>

	<div id="menu">
				<div class="menu"><ul>
<li><a href="https://studywolf.wordpress.com/">Home</a></li><li class="page_item page-item-2"><a href="https://studywolf.wordpress.com/about/">About</a></li>
<li class="page_item page-item-7615"><a href="https://studywolf.wordpress.com/site-index/">Site Index</a></li>
</ul></div>
	</div>

	<div id="contents">
					
				
<div class="post-1043 post type-post status-publish format-standard hentry category-learning category-programming category-python category-reinforcement-learning tag-python tag-reinforcement-learning-2 tag-sarsa" id="post-1043">
	<div class="entry-meta">
				<div class="date"><a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/">Jul 01 2013</a></div>
						<div class="comments"><a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comments">33 Comments</a></div>
						<span class="byline">
			<a href="https://studywolf.wordpress.com/author/travisdewolf/">
				By travisdewolf			</a>
		</span><!-- .byline -->
				<span class="cat-links"><a href="https://studywolf.wordpress.com/category/learning/" rel="category tag">Learning</a>, <a href="https://studywolf.wordpress.com/category/programming/" rel="category tag">programming</a>, <a href="https://studywolf.wordpress.com/tag/python/" rel="category tag">Python</a>, <a href="https://studywolf.wordpress.com/category/learning/reinforcement-learning/" rel="category tag">Reinforcement Learning</a></span>
							</div>
	<div class="main">
						<h2 class="entry-title">
					Reinforcement Learning part 2: SARSA vs&nbsp;Q-learning				</h2>
		
		<div class="entry-content">
						<p>In my <a href="https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/">previous post about reinforcement learning</a> I talked about Q-learning, and how that works in the context of a cat vs mouse game. I mentioned in this post that there are a number of other methods of reinforcement learning aside from Q-learning, and today I’ll talk about another one of them: SARSA. All the code used is from Terry Stewart’s <a href="https://github.com/tcstewar/ccmsuite">RL code repository</a>, and can be found both there and in a minimalist version on my own github: <a href="https://github.com/studywolf/blog/tree/master/RL/SARSA%20vs%20Qlearn%20cliff">SARSA vs Qlearn cliff</a>. To run the code, simply execute the <code>cliff_Q</code> or the <code>cliff_S</code> files.</p>
<p>SARSA stands for State-Action-Reward-State-Action. In SARSA, the agent starts in state 1, performs action 1, and gets a reward (reward 1). Now, it’s in state 2 and performs another action (action 2) and gets the reward from this state (reward 2) before it goes back and updates the value of action 1 performed in state 1. In contrast, in Q-learning the agent starts in state 1, performs action 1 and gets a reward (reward 1), and then looks and sees what the maximum possible reward for an action is in state 2, and uses that to update the action value of performing action 1 in state 1. So the difference is in the way the future reward is found. In Q-learning it’s simply the highest possible action that can be taken from state 2, and in SARSA it’s the value of the <em>actual</em> action that was taken.</p>
<p>This means that SARSA takes into account the control policy by which the agent is moving, and incorporates that into its update of action values, where Q-learning simply assumes that an optimal policy is being followed. This difference can be a little difficult conceptually to tease out at first but with an example will hopefully become clear.</p>
<p><b>Mouse vs cliff</b></p>
<p>Let’s look at a simple scenario, a mouse is trying to get to a piece of cheese. Additionally, there is a cliff in the map that must be avoided, or the mouse falls, gets a negative reward, and has to start back at the beginning. The simulation looks something like exactly like this:</p>
<p><a href="https://studywolf.files.wordpress.com/2013/06/mouse-vs-cliff.png"><img loading="lazy" data-attachment-id="1046" data-permalink="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/mouse-vs-cliff/" data-orig-file="https://studywolf.files.wordpress.com/2013/06/mouse-vs-cliff.png" data-orig-size="423,185" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="mouse vs cliff" data-image-description="" data-image-caption="" data-medium-file="https://studywolf.files.wordpress.com/2013/06/mouse-vs-cliff.png?w=300" data-large-file="https://studywolf.files.wordpress.com/2013/06/mouse-vs-cliff.png?w=423" class="aligncenter size-medium wp-image-1046" alt="mouse vs cliff" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/mouse-vs-cliff.png" width="300" height="131" srcset="https://studywolf.files.wordpress.com/2013/06/mouse-vs-cliff.png?w=300&amp;h=131 300w, https://studywolf.files.wordpress.com/2013/06/mouse-vs-cliff.png?w=150&amp;h=66 150w, https://studywolf.files.wordpress.com/2013/06/mouse-vs-cliff.png 423w" sizes="(max-width: 300px) 100vw, 300px"></a><br>
where the black is the edge of the map (walls), the red is the cliff area, the blue is the mouse and the green is the cheese. As mentioned and linked to above, the code for all of these examples can be found <a href="https://github.com/studywolf/blog/tree/master/RL/SARSA%20vs%20Qlearn%20cliff">on my github</a> (as a side note: when using the github code remember that you can press the page-up and page-down buttons to speed up and slow down the rate of simulation!)</p>
<p>Now, as we all remember, in the basic Q-learning control policy the action to take is chosen by having the highest action value. However, there is also a chance that some random action will be chosen; this is the built-in exploration mechanism of the agent. This means that even if we see this scenario:</p>
<p><a href="https://studywolf.files.wordpress.com/2013/06/mouse-and-cliff.jpeg"><img loading="lazy" data-attachment-id="1048" data-permalink="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/mouse-and-cliff/" data-orig-file="https://studywolf.files.wordpress.com/2013/06/mouse-and-cliff.jpeg" data-orig-size="462,223" data-comments-opened="1" data-image-meta="{&quot;aperture&quot;:&quot;0&quot;,&quot;credit&quot;:&quot;&quot;,&quot;camera&quot;:&quot;&quot;,&quot;caption&quot;:&quot;&quot;,&quot;created_timestamp&quot;:&quot;0&quot;,&quot;copyright&quot;:&quot;&quot;,&quot;focal_length&quot;:&quot;0&quot;,&quot;iso&quot;:&quot;0&quot;,&quot;shutter_speed&quot;:&quot;0&quot;,&quot;title&quot;:&quot;&quot;}" data-image-title="mouse and cliff" data-image-description="" data-image-caption="" data-medium-file="https://studywolf.files.wordpress.com/2013/06/mouse-and-cliff.jpeg?w=300" data-large-file="https://studywolf.files.wordpress.com/2013/06/mouse-and-cliff.jpeg?w=462" class="aligncenter size-medium wp-image-1048" alt="mouse and cliff" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/mouse-and-cliff.jpeg" width="300" height="144" srcset="https://studywolf.files.wordpress.com/2013/06/mouse-and-cliff.jpeg?w=298&amp;h=144 298w, https://studywolf.files.wordpress.com/2013/06/mouse-and-cliff.jpeg?w=150&amp;h=72 150w, https://studywolf.files.wordpress.com/2013/06/mouse-and-cliff.jpeg 462w" sizes="(max-width: 300px) 100vw, 300px"></a><br>
There is a chance that that mouse is going to say ‘yes I see the best move, but…the hell with it’ and jump over the edge! All in the name of exploration. This becomes a problem, because if the mouse was following an optimal control strategy, it would simply run right along the edge of the cliff all the way over to the cheese and grab it. Q-learning assumes that the mouse is following the optimal control strategy, so the action values will converge such that the best path is along the cliff. Here’s an animation of the result of running the Q-learning code for a long time:</p>
<p><a href="https://studywolf.files.wordpress.com/2013/07/optimised.gif"><img class="aligncenter" alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/optimised.gif"></a><br>
The solution that the mouse ends up with is running along the edge of the cliff and occasionally jumping off and plummeting to its death.</p>
<p>However, if the agent’s <em>actual</em> control strategy is taken into account when learning, something very different happens. Here is the result of the mouse learning to find its way to the cheese using SARSA:</p>
<p><a href="https://studywolf.files.wordpress.com/2013/07/sarsa-optimised.gif"><img class="aligncenter" alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/sarsa-optimised.gif"></a><br>
Why, that’s <em>much</em> better! The mouse has learned that from time to time it does really foolish things, so the best path is not to run along the edge of the cliff straight to the cheese but to get far away from the cliff and then work its way over safely. As you can see, even if a random action is chosen there is little chance of it resulting in death.</p>
<p><b>Learning action values with SARSA</b></p>
<p>So now we know how SARSA determines it’s updates to the action values. It’s a very minor difference between the SARSA and Q-learning implementations, but it causes a profound effect.</p>
<p>Here is the Q-learning <code>learn</code> method:</p>
<div><div id="highlighter_718425" class="syntaxhighlighter  python"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td class="gutter"><div class="line number40 index0 alt1">40</div><div class="line number41 index1 alt2">41</div><div class="line number42 index2 alt1">42</div><div class="line number43 index3 alt2">43</div></td><td class="code"><div class="container"><div class="line number40 index0 alt1"><code class="python keyword">def</code> <code class="python plain">learn(</code><code class="python color1">self</code><code class="python plain">, state1, action1, reward, state2):</code></div><div class="line number41 index1 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">maxqnew </code><code class="python keyword">=</code> <code class="python functions">max</code><code class="python plain">([</code><code class="python color1">self</code><code class="python plain">.getQ(state2, a) </code><code class="python keyword">for</code> <code class="python plain">a </code><code class="python keyword">in</code> <code class="python color1">self</code><code class="python plain">.actions])</code></div><div class="line number42 index2 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python color1">self</code><code class="python plain">.learnQ(state1, action1,</code></div><div class="line number43 index3 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">reward, reward </code><code class="python keyword">+</code> <code class="python color1">self</code><code class="python plain">.gamma</code><code class="python keyword">*</code><code class="python plain">maxqnew)</code></div></div></td></tr></tbody></table></div></div>
<p>And here is the SARSA <code>learn</code> method</p>
<div><div id="highlighter_311178" class="syntaxhighlighter  python"><table border="0" cellpadding="0" cellspacing="0"><tbody><tr><td class="gutter"><div class="line number39 index0 alt2">39</div><div class="line number40 index1 alt1">40</div><div class="line number41 index2 alt2">41</div><div class="line number42 index3 alt1">42</div></td><td class="code"><div class="container"><div class="line number39 index0 alt2"><code class="python keyword">def</code> <code class="python plain">learn(</code><code class="python color1">self</code><code class="python plain">, state1, action1, reward, state2, action2):</code></div><div class="line number40 index1 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">qnext </code><code class="python keyword">=</code> <code class="python color1">self</code><code class="python plain">.getQ(state2, action2)</code></div><div class="line number41 index2 alt2"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python color1">self</code><code class="python plain">.learnQ(state1, action1,</code></div><div class="line number42 index3 alt1"><code class="python spaces">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</code><code class="python plain">reward, reward </code><code class="python keyword">+</code> <code class="python color1">self</code><code class="python plain">.gamma </code><code class="python keyword">*</code> <code class="python plain">qnext)</code></div></div></td></tr></tbody></table></div></div>
<p>As we can see, the SARSA method takes another parameter, <code>action2</code>, which is the action that was taken by the agent from the second state. This allows the agent to explicitly find the future reward value, <code>qnext</code>, that followed, rather than assuming that the optimal action will be taken and that the largest reward, <code>maxqnew</code>, resulted.</p>
<p>Written out, the Q-learning update policy is <code>Q(s, a) = reward(s) + alpha * max(Q(s'))</code>, and the SARSA update policy is <code>Q(s, a) = reward(s) + alpha * Q(s', a')</code>. This is how SARSA is able to take into account the control policy of the agent during learning. It means that information needs to be stored longer before the action values can be updated, but also means that our mouse is going to jump off a cliff much less frequently, which we can probably all agree is a good thing.</p>
<div id="atatags-370373-6131defa2d49a">
        <script type="text/javascript">
            __ATA.cmd.push(function() {
                __ATA.initVideoSlot('atatags-370373-6131defa2d49a', {
                    sectionId: '370373',
                    format: 'inread'
                });
            });
        </script>
    </div>			<div id="atatags-26942-6131defa2d4d3"></div>
			
			<script>
				__ATA.cmd.push(function() {
					__ATA.initDynamicSlot({
						id: 'atatags-26942-6131defa2d4d3',
						location: 120,
						formFactor: '001',
						label: {
							text: 'Advertisements',
						},
						creative: {
							reportAd: {
								text: 'Report this ad',
							},
							privacySettings: {
								text: 'Privacy',
								onClick: function() { window.__tcfapi && window.__tcfapi( 'showUi' ); },
							}
						}
					});
				});
			</script><div id="jp-post-flair" class="sharedaddy sd-like-enabled sd-sharing-enabled"><div class="sharedaddy sd-sharing-enabled"><div class="robots-nocontent sd-block sd-social sd-social-icon-text sd-sharing"><h3 class="sd-title">Share this:</h3><div class="sd-content"><ul data-sharing-events-added="true"><li class="share-twitter"><a rel="nofollow noopener noreferrer" data-shared="sharing-twitter-1043" class="share-twitter sd-button share-icon" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?share=twitter&amp;nb=1" target="_blank" title="Click to share on Twitter"><span>Twitter</span></a></li><li class="share-facebook"><a rel="nofollow noopener noreferrer" data-shared="sharing-facebook-1043" class="share-facebook sd-button share-icon" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?share=facebook&amp;nb=1" target="_blank" title="Click to share on Facebook"><span>Facebook</span></a></li><li class="share-end"></li></ul></div></div></div><div class="sharedaddy sd-block sd-like jetpack-likes-widget-wrapper jetpack-likes-widget-loaded" id="like-post-wrapper-39795907-1043-6131defa2e050" data-src="//widgets.wp.com/likes/index.html?ver=20210818#blog_id=39795907&amp;post_id=1043&amp;origin=studywolf.wordpress.com&amp;obj_id=39795907-1043-6131defa2e050" data-name="like-post-frame-39795907-1043-6131defa2e050" data-title="Like or Reblog"><h3 class="sd-title">Like this:</h3><div class="likes-widget-placeholder post-likes-widget-placeholder" style="height: 55px; display: none;"><span class="button"><span>Like</span></span> <span class="loading">Loading...</span></div><iframe class="post-likes-widget jetpack-likes-widget" name="like-post-frame-39795907-1043-6131defa2e050" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/index.html" height="55px" width="100%" frameborder="0" title="Like or Reblog"></iframe><span class="sd-text-color"></span><a class="sd-link-color"></a></div>
<div id="jp-relatedposts" class="jp-relatedposts" style="display: block;">
	<h3 class="jp-relatedposts-headline"><em>Related</em></h3>
<div class="jp-relatedposts-items jp-relatedposts-items-minimal jp-relatedposts-grid "><p class="jp-relatedposts-post jp-relatedposts-post0" data-post-id="410" data-post-format="false"><span class="jp-relatedposts-post-title"><a class="jp-relatedposts-post-a" href="https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/?relatedposts_hit=1&amp;relatedposts_origin=1043&amp;relatedposts_position=0" title="Reinforcement learning part 1: Q-learning and exploration" data-origin="1043" data-position="0">Reinforcement learning part 1: Q-learning and&nbsp;exploration</a></span><time class="jp-relatedposts-post-date" datetime="November 25, 2012" style="display: block;">November 25, 2012</time><span class="jp-relatedposts-post-context">In "Learning"</span></p><p class="jp-relatedposts-post jp-relatedposts-post1" data-post-id="1044" data-post-format="false"><span class="jp-relatedposts-post-title"><a class="jp-relatedposts-post-a" href="https://studywolf.wordpress.com/2015/03/29/reinforcement-learning-part-3-egocentric-learning/?relatedposts_hit=1&amp;relatedposts_origin=1043&amp;relatedposts_position=1" title="Reinforcement Learning part 3: Egocentric learning" data-origin="1043" data-position="1">Reinforcement Learning part 3: Egocentric&nbsp;learning</a></span><time class="jp-relatedposts-post-date" datetime="March 29, 2015" style="display: block;">March 29, 2015</time><span class="jp-relatedposts-post-context">In "Learning"</span></p><p class="jp-relatedposts-post jp-relatedposts-post2" data-post-id="2321" data-post-format="false"><span class="jp-relatedposts-post-title"><a class="jp-relatedposts-post-a" href="https://studywolf.wordpress.com/2015/04/09/reinforcement-learning-part-4-combining-egocentric-and-allocentric/?relatedposts_hit=1&amp;relatedposts_origin=1043&amp;relatedposts_position=2" title="Reinforcement Learning part 4: Combining egocentric and allocentric" data-origin="1043" data-position="2">Reinforcement Learning part 4: Combining egocentric and&nbsp;allocentric</a></span><time class="jp-relatedposts-post-date" datetime="April 9, 2015" style="display: block;">April 9, 2015</time><span class="jp-relatedposts-post-context">In "Learning"</span></p></div></div></div>		</div>
		<span class="tag-links"><strong>Tagged</strong> <a href="https://studywolf.wordpress.com/tag/python/" rel="tag">Python</a>, <a href="https://studywolf.wordpress.com/tag/reinforcement-learning-2/" rel="tag">reinforcement learning</a>, <a href="https://studywolf.wordpress.com/tag/sarsa/" rel="tag">SARSA</a></span>	</div>
</div>


<div id="comments">

			<h2 id="comments-title">
			33 thoughts on “<span>Reinforcement Learning part 2: SARSA vs&nbsp;Q-learning</span>”		</h2>

		<ol class="commentlist">
					<li class="comment even thread-even depth-1 highlander-comment" id="comment-856">
				<div id="div-comment-856" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/a64192cf9c1f5e91ffc83577918b610e" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-a64192cf9c1f5e91ffc83577918b610e-0">			<cite class="fn">Hossein Rafipoor</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-856">January 27, 2014 at 4:35 pm</a>		</div>

		<p>thank you for your brief and explicit notes, that was helpful!<br>
but i have a question, do you claiming that Sarsa is Better than QLearning in any aspect?!<br>
suppose you have little information about environment and have a little time , e.g. you have few number of episodes to live in Environment, which one you choose for your agent? Sarsa or Qlearning?</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=856#respond" data-commentid="856" data-postid="1043" data-belowelement="div-comment-856" data-respondelement="respond" data-replyto="Reply to Hossein Rafipoor" aria-label="Reply to Hossein Rafipoor">Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-travisdewolf bypostauthor odd alt thread-odd thread-alt depth-1 parent highlander-comment" id="comment-859">
				<div id="div-comment-859" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/15812e89d7ec393690252d77ea02bf3d.png" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-15812e89d7ec393690252d77ea02bf3d-0">			<cite class="fn"><a href="https://studywolf.wordpress.com/" rel="external nofollow ugc" class="url">travisdewolf</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-859">January 29, 2014 at 3:51 pm</a>		</div>

		<p>Thanks! Glad you enjoyed them.</p>
<p>I wouldn’t say that SARSA is is better than Q-learning in every aspect, but the benefit of it is that it takes into account what your actual system policy, rather than just assuming that you’re doing the right thing. But if you have a system where your policy is changing a lot it could be much more desirable to use the Q-learning approach and learn assuming that you’re moving optimally, and then incorporate that however you will, rather than having a SARSA system that tries to account for your constant changing movement policy. </p>
<p>In a scenario where you only have a few episodes for learning there’s no real difference between the two, as neither was built to address that situation more efficiently than the other.</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=859#respond" data-commentid="859" data-postid="1043" data-belowelement="div-comment-859" data-respondelement="respond" data-replyto="Reply to travisdewolf" aria-label="Reply to travisdewolf">Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-2 parent highlander-comment" id="comment-860">
				<div id="div-comment-860" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/a64192cf9c1f5e91ffc83577918b610e" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-a64192cf9c1f5e91ffc83577918b610e-1">			<cite class="fn">Hossein Rafipoor</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-860">January 29, 2014 at 5:26 pm</a>		</div>

		<p>Thank you, Glad to find you!;)<br>
but i was thinking QLearning may be better in this scenario, because something that you mentioned above:<br>
” But if you have a system where your policy is changing a lot it could be much more desirable to use the Q-learning approach and learn assuming that you’re moving optimally, and then incorporate that however you will, rather than having a SARSA system that tries to account for your constant changing movement policy”<br>
i think in very first episodes the agents knowledge is too few, so it will changes its policy too much, so in first episodes QLearning may be better, in most of problems, how ever this may depend on very other things in general, specially Environment properties.<br>
in other words i guess QLearning agent will find a sub optimal policy faster,and slowly will changes it toward the optimal policy, but SARSA will find a better policy after a few steps more, when it finds a little knowledge about Environment. how ever i have not enough implementations to test this, i have tested that on one problem and that seems to be true.<br>
what do you think about this?!</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=860#respond" data-commentid="860" data-postid="1043" data-belowelement="div-comment-860" data-respondelement="respond" data-replyto="Reply to Hossein Rafipoor" aria-label="Reply to Hossein Rafipoor">Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-travisdewolf bypostauthor odd alt depth-3 highlander-comment" id="comment-861">
				<div id="div-comment-861" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/15812e89d7ec393690252d77ea02bf3d.png" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-15812e89d7ec393690252d77ea02bf3d-1">			<cite class="fn"><a href="https://studywolf.wordpress.com/" rel="external nofollow ugc" class="url">travisdewolf</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-861">January 31, 2014 at 11:36 am</a>		</div>

		<p>I see, I think you might right that you will get a slightly better set of Q-values out of Q-learning rather than SARSA. This could be because Q-learning is ‘looking into the future’ during Q-value assignment (what is the reward from the best possible path from here), whereas SARSA is assigning Q-values based on much more immediate information (what is the reward from what I just did). </p>
<p>That’s not to say that this is because the control policy is changing (it would be the same policy of greedy search with chance of exploration the whole time), but because the mouse ‘gets scared’ of the cliff and takes longer to find a path to the cheese.</p>
<p>So I’ve convinced myself that this makes sense, but it would be a neat thing to test for a small project! Wouldn’t required much modification of the code already there, please let me know if you run it what the results are!</p>

		
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1 parent highlander-comment" id="comment-981">
				<div id="div-comment-981" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/b2f24b53caabf7ee37a2a2ab182f25a2" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-b2f24b53caabf7ee37a2a2ab182f25a2-0">			<cite class="fn">John</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-981">May 3, 2014 at 7:36 pm</a>		</div>

		<p>Love the illustration — This post went a long way towards helping me understand the practical differences in these two implementations.  Thanks!</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=981#respond" data-commentid="981" data-postid="1043" data-belowelement="div-comment-981" data-respondelement="respond" data-replyto="Reply to John" aria-label="Reply to John">Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-travisdewolf bypostauthor odd alt depth-2 highlander-comment" id="comment-982">
				<div id="div-comment-982" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/15812e89d7ec393690252d77ea02bf3d.png" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-15812e89d7ec393690252d77ea02bf3d-2">			<cite class="fn"><a href="https://studywolf.wordpress.com/" rel="external nofollow ugc" class="url">travisdewolf</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-982">May 5, 2014 at 9:07 am</a>		</div>

		<p>That’s great! Glad to hear you enjoyed the post, thanks for the comment!</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=982#respond" data-commentid="982" data-postid="1043" data-belowelement="div-comment-982" data-respondelement="respond" data-replyto="Reply to travisdewolf" aria-label="Reply to travisdewolf">Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-odd thread-alt depth-1 highlander-comment" id="comment-1279">
				<div id="div-comment-1279" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/picture" class="avatar avatar-32" height="32" width="32">			<cite class="fn"><a href="https://www.facebook.com/verhack" rel="external nofollow ugc" class="url">Ruben Verhack</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-1279">January 12, 2015 at 10:06 am</a>		</div>

		<p>Thank you for this elaborate explanation!</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=1279#respond" data-commentid="1279" data-postid="1043" data-belowelement="div-comment-1279" data-respondelement="respond" data-replyto="Reply to Ruben Verhack" aria-label="Reply to Ruben Verhack">Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-tkim049 odd alt thread-even depth-1 highlander-comment" id="comment-1415">
				<div id="div-comment-1415" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/602683f4544da7e81edab7ca3c0e49b6.png" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-602683f4544da7e81edab7ca3c0e49b6-0">			<cite class="fn"><a href="http://www.tykim.me/" rel="external nofollow ugc" class="url">Taeyoung</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-1415">July 8, 2015 at 6:29 pm</a>		</div>

		<p>Reblogged this on <a href="http://tykim.me/2015/07/08/reinforcement-learning-part-2-sarsa-vs-q-learning/" rel="nofollow">Taeyoung Kim</a> and commented:<br>
Things to learn</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=1415#respond" data-commentid="1415" data-postid="1043" data-belowelement="div-comment-1415" data-respondelement="respond" data-replyto="Reply to Taeyoung" aria-label="Reply to Taeyoung">Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment even thread-odd thread-alt depth-1 parent highlander-comment" id="comment-1748">
				<div id="div-comment-1748" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/bca24b3fc0c716ffaf8fd3f0138050c0" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-bca24b3fc0c716ffaf8fd3f0138050c0-0">			<cite class="fn">Marie</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-1748">March 29, 2016 at 5:57 am</a>		</div>

		<p>Great example and animation, it really helped me grasp the difference (or so I hope).<br>
However I have one question or remark.</p>
<p>So in both cases we learn a behavior by experience and then proceed acting based on what we have learned. While learning we try to explore by making stupid decisions now and then, Because sometimes we might learn the decision wasn’t as stupid as we initially thought (maybe there’s more cheese at the bottom of that cliff?)</p>
<p>As seen in the animations the value function found by SARSA assumes we will keep on acting stupid now and then, the value function found by Q-learning assumes we don’t ever act stupid. </p>
<p>But wouldn’t it be beneficial to just stop acting stupid at some point? This obviously depends on the problem we are trying to solve. But say we wanted to teach a robot to bring first aid to wounded humans and avoid mines along the way. We have it learn and experiment for as long as we can in a controlled, non-explosive environment. But once it is actually used in action we prevent it from exploring because we don’t want it to randomly roll into a mine in the name of science.</p>
<p>With Q-learning we actually found the optimal policy of running straight to the cheese while SARSA taught the robot the optimal policy of avoiding its suicidal fits while trying to eventually get to the cheese. The latter would be awful for our no-longer-suicidal first aid robot.</p>
<p>Then again the reward in the mouse example didn’t specify that we were in a hurry. Assuming no time limit or energy expended by moving the SARSA policy is always better as long as we explore and both policies are equally good once we stop exploring.</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=1748#respond" data-commentid="1748" data-postid="1043" data-belowelement="div-comment-1748" data-respondelement="respond" data-replyto="Reply to Marie" aria-label="Reply to Marie">Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-travisdewolf bypostauthor odd alt depth-2 highlander-comment" id="comment-1760">
				<div id="div-comment-1760" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/15812e89d7ec393690252d77ea02bf3d.png" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-15812e89d7ec393690252d77ea02bf3d-3">			<cite class="fn"><a href="https://studywolf.wordpress.com/" rel="external nofollow ugc" class="url">travisdewolf</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-1760">April 5, 2016 at 10:54 am</a>		</div>

		<p>Hi Marie, </p>
<p>so i’ll rephrase ‘stupid decisions’ as ‘testing out sub-optimal decisions in case our assigned value of a decision is incorrect’. <img draggable="false" role="img" class="emoji" alt="😀" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/1f600.svg"><br>
<a href="https://studywolf.wordpress.com/2012/11/25/reinforcement-learning-q-learning-and-exploration/" rel="nofollow">In my first RL post</a> I explore reducing the chances of randomly choosing a direction for exploration over time, and the results are pretty good! Also, a common practice is to train up the system and then just turn exploration off entirely, like you suggested! So definitely it’s a practice that’s been shown to work well. </p>
<p>Incorporating time / urgency into the problem would be an interesting little project as well, you could have another dimension of the state represent how long the mouse has been around that penalizes large numbers, which would lead to an interesting balance between running alongside the cliff and taking a more conservative / safer path. </p>
<p>But you’re right on both counts!</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=1760#respond" data-commentid="1760" data-postid="1043" data-belowelement="div-comment-1760" data-respondelement="respond" data-replyto="Reply to travisdewolf" aria-label="Reply to travisdewolf">Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1 parent highlander-comment" id="comment-1754">
				<div id="div-comment-1754" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/29a142f7a31ab6203692a702f37fdf11" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-29a142f7a31ab6203692a702f37fdf11-0">			<cite class="fn">Gil Yaary</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-1754">April 1, 2016 at 1:38 pm</a>		</div>

		<p>After reading a lot of stuff about SARSA vs Q-Learning your post made it clear. The quotation that makes the difference clear is:</p>
<p>“In Q-learning it’s simply the highest possible action that can be taken from state 2, and in SARSA it’s the value of the actual action that was taken.</p>
<p>This means that SARSA takes into account the control policy by which the agent is moving, and incorporates that into its update of action values, where Q-learning simply assumes that an optimal policy is being followed”</p>
<p>So my understanding is that SARSA takes a CURRENT POLICY and follows it to calculate the state-action values. So it EVALUATES a non-optimal but current policy. Q-Learning keeps on changing the policy while evaluating it.</p>
<p>Now my questions are:<br>
1. At which point does SARSA explores and attempts to find a better policy?<br>
2. How does SARSA know that it has a good approximation of the State Action values of the policy at-hand (current policy)?<br>
3. Is there a gradient between pure Q-Learning and SARSA?</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=1754#respond" data-commentid="1754" data-postid="1043" data-belowelement="div-comment-1754" data-respondelement="respond" data-replyto="Reply to Gil Yaary" aria-label="Reply to Gil Yaary">Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-travisdewolf bypostauthor odd alt depth-2 highlander-comment" id="comment-1761">
				<div id="div-comment-1761" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/15812e89d7ec393690252d77ea02bf3d.png" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-15812e89d7ec393690252d77ea02bf3d-4">			<cite class="fn"><a href="https://studywolf.wordpress.com/" rel="external nofollow ugc" class="url">travisdewolf</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-1761">April 5, 2016 at 11:07 am</a>		</div>

		<p>Hmmm so it’s not quite that Q-Learning changes the policy while evaluating and SARSA doesn’t, they’re both using the most recent information to make decisions. In fact, SARSA even fluctuates a bit more in that sense because it uses the actual trajectory taken to update weights (which may not have been a good decision), rather than Q-learning which just assumes that the best decision is always made. But yes, you’re right SARSA evaluates the current policy rather than assuming an optimal one is being used, like Q-learning does. Alright questions! </p>
<p>1) Both Q-learning and SARSA are for learning information about the environment, and separated from the actual policy that makes decisions. They learn ‘hey if you do this, based on what’s happened before it’s probably about this rewarding’, and then there’s some separate system, uninvolved from the learning that both algorithms do, that defines the control policy and makes the call on which action to take.<br>
2) It does not! It just knows ‘you were there and you went here’, and updates the action values based on this information. It could be a once in a million action but it will update the weights all the same. But as that movement will happen much less than others you’ll end up with a good set of action values given the control policy because as things happen more or less often the associated action values will change accordingly.<br>
3) Hmmm none that I’m aware of! But that’s not saying all too much. It’s a pretty discrete difference between the two. Aside from alternating between Q-learning and SARSA updating nothing comes to mind on combinations. </p>
<p>Hope that helps!</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=1761#respond" data-commentid="1761" data-postid="1043" data-belowelement="div-comment-1761" data-respondelement="respond" data-replyto="Reply to travisdewolf" aria-label="Reply to travisdewolf">Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-odd thread-alt depth-1 parent highlander-comment" id="comment-1823">
				<div id="div-comment-1823" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/b27b1ea94ea1c641fb621a21738ea631" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-b27b1ea94ea1c641fb621a21738ea631-0">			<cite class="fn">Sridhar</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-1823">May 30, 2016 at 8:50 am</a>		</div>

		<p>When we turn off exploration, it will no longer make random moves while moving on the cliff right..so when we turn off exploration the policy found by q learning is better..so in what sense are we right to say SARSA has found us a better solution.In the long run dont we want the most optimal policy?</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=1823#respond" data-commentid="1823" data-postid="1043" data-belowelement="div-comment-1823" data-respondelement="respond" data-replyto="Reply to Sridhar" aria-label="Reply to Sridhar">Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-travisdewolf bypostauthor odd alt depth-2 highlander-comment" id="comment-1839">
				<div id="div-comment-1839" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/15812e89d7ec393690252d77ea02bf3d.png" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-15812e89d7ec393690252d77ea02bf3d-5">			<cite class="fn"><a href="https://studywolf.wordpress.com/" rel="external nofollow ugc" class="url">travisdewolf</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-1839">June 20, 2016 at 2:10 pm</a>		</div>

		<p>Hi Sridhar, </p>
<p>hmm, so there are a few cases where SARSA comes to a better solution. One is we don’t want to turn off learning, for example if it’s possible that the environment is changing. In that case exploration is always part of our policy and the better solution accounts for that. Other reasons for deviations from the greedy policy (that q-learning assumes) could be that the agent has some restrictions placed upon it (for example: it can’t turn left) or other things that a simple greedy algorithm doesn’t take into account. You q-learner might also not have access to all of the information that the control policy does, so what is a greedy optimization to the q-learner is a very poor action choice for the policy. Hope these examples help clarify benefits of SARSA! <img draggable="false" role="img" class="emoji" alt="🙂" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/1f642.svg"></p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=1839#respond" data-commentid="1839" data-postid="1043" data-belowelement="div-comment-1839" data-respondelement="respond" data-replyto="Reply to travisdewolf" aria-label="Reply to travisdewolf">Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1 parent highlander-comment" id="comment-1960">
				<div id="div-comment-1960" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/0215713b62e3606c3d2ec991824eb754" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-0215713b62e3606c3d2ec991824eb754-0">			<cite class="fn">Shiza</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-1960">September 26, 2016 at 10:28 pm</a>		</div>

		<p>I want to ask that SARSA can be used for routing the data? If I want to use it as routing protocol in Wireless Sensor Networks then Will it be suitable or possible to implement?</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=1960#respond" data-commentid="1960" data-postid="1043" data-belowelement="div-comment-1960" data-respondelement="respond" data-replyto="Reply to Shiza" aria-label="Reply to Shiza">Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-travisdewolf bypostauthor odd alt depth-2 parent highlander-comment" id="comment-1963">
				<div id="div-comment-1963" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/15812e89d7ec393690252d77ea02bf3d.png" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-15812e89d7ec393690252d77ea02bf3d-6">			<cite class="fn"><a href="https://studywolf.wordpress.com/" rel="external nofollow ugc" class="url">travisdewolf</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-1963">September 27, 2016 at 11:21 am</a>		</div>

		<p>Hi Shiza, </p>
<p>hmm, it’s hard to say, without knowing more about the problem. We’d need to know what the state representation would be, what the possible actions are from each state, what the reward for different state / actions would be. I suspect, however, that SARSA isn’t the best choice for this problem!</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=1963#respond" data-commentid="1963" data-postid="1043" data-belowelement="div-comment-1963" data-respondelement="respond" data-replyto="Reply to travisdewolf" aria-label="Reply to travisdewolf">Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment even depth-3 parent highlander-comment" id="comment-1986">
				<div id="div-comment-1986" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/0215713b62e3606c3d2ec991824eb754" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-0215713b62e3606c3d2ec991824eb754-1">			<cite class="fn"><a href="https://studywolf.wordpress.com/" rel="external nofollow ugc" class="url">Shiza</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-1986">October 11, 2016 at 8:55 pm</a>		</div>

		<p>In my scenario , I have 50 randomly deployed node with a battery recharging device moving in the WSN environment. With help of that device WSN can remain operational longer, I want to use SARSA for data routing among those nodes so that energy consumption can be reduced. So Can we use it as routing protocol?</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-travisdewolf bypostauthor odd alt depth-3 parent highlander-comment" id="comment-1988">
				<div id="div-comment-1988" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/15812e89d7ec393690252d77ea02bf3d.png" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-15812e89d7ec393690252d77ea02bf3d-7">			<cite class="fn"><a href="https://studywolf.wordpress.com/" rel="external nofollow ugc" class="url">travisdewolf</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-1988">October 12, 2016 at 12:45 pm</a>		</div>

		<p>Hi Shiza,<br>
hm, so you want to avoid routing data through low-energy nodes? You could set this up as an RL problem, but I’m not sure it’d be a great choice.<br>
To start you’d need to define a state space, possible actions from each state, and a cost function, what would those look like?</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment even depth-3 parent highlander-comment" id="comment-1991">
				<div id="div-comment-1991" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/0215713b62e3606c3d2ec991824eb754" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-0215713b62e3606c3d2ec991824eb754-2">			<cite class="fn"><a href="https://studywolf.wordpress.com/" rel="external nofollow ugc" class="url">Shiza</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-1991">October 13, 2016 at 3:22 am</a>		</div>

		<p>yes Exactly, But I am confused How Can I assign the values for state, action…in WSN. Is a node can be a state with the measuring of remaining energy? action can be like to decide which node can send data on the basis of remaining battery capacity? or any other thing m just totaly confused could you please help me?</p>

		
				</div>
				</li><!-- #comment-## -->
		<li class="comment byuser comment-author-travisdewolf bypostauthor odd alt depth-3 highlander-comment" id="comment-2020">
				<div id="div-comment-2020" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/15812e89d7ec393690252d77ea02bf3d.png" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-15812e89d7ec393690252d77ea02bf3d-8">			<cite class="fn"><a href="https://studywolf.wordpress.com/" rel="external nofollow ugc" class="url">travisdewolf</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-2020">November 15, 2016 at 10:37 am</a>		</div>

		<p>Hi Shiza, </p>
<p>there’s a bunch of different ways that you could choose to set up the state space and possible actions, etc, it’s often a large part of the solution design. If you want to learn which neighbouring node to send data to you’ll want your state space to incorporate all the information required to make that decision. If you think of your nodes as different grid points on a map that you’re learning about, the initial node as your starting state and the target node as your cheese, then you’re essentially learning the best way to move to the cheese. It’s a more complex problem though, because 1) which node has the cheese is changing, 2) which path is optimal is constantly changing because of the power levels of each node. So in some fashion you probably need to include in your system state information about where the target is and the power levels of the nodes in the map. It expands the state space quite a bit and makes the problem much harder. </p>
<p>Off the top of my head, I would probably make an ego-centric RL agent to learn passing the information to neighbour nodes with high-energy, and an allocentric RL agent to learn the map of the network, and reward passing to nodes that are closer to the target. You could then combine the actions weightings from each to choose which action to take at a given state (which node to send the packet to). Something like that might work! Good luck!</p>

		
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-odd thread-alt depth-1 parent highlander-comment" id="comment-2156">
				<div id="div-comment-2156" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/picture(1)" class="avatar avatar-32" height="32" width="32">			<cite class="fn"><a href="https://www.facebook.com/app_scoped_user_id/10154237045912414/" rel="external nofollow ugc" class="url">Fabio Zinno</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-2156">March 4, 2017 at 8:46 pm</a>		</div>

		<p>Hi there, thanks for the great posts!<br>
I actually just finished reading the Temporal Difference Chapter of Sutton’s book, so I’m far from an expert in the subject.<br>
Reading the post though, the first thing I thought is this: SARSA’s solution is not finding the optimal solution in the mouse-cheese problem. The optimal policy is to run along the edge, isn’t it?<br>
I’d rather run q-learning until convergence, and when the optimal policy is found, the run-time agent would simply remove the e-greedy part of it and will actually avoid falling into the cliff.<br>
What am I missing here?</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=2156#respond" data-commentid="2156" data-postid="1043" data-belowelement="div-comment-2156" data-respondelement="respond" data-replyto="Reply to Fabio Zinno" aria-label="Reply to Fabio Zinno">Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-travisdewolf bypostauthor odd alt depth-2 highlander-comment" id="comment-2176">
				<div id="div-comment-2176" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/15812e89d7ec393690252d77ea02bf3d.png" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-15812e89d7ec393690252d77ea02bf3d-9">			<cite class="fn"><a href="https://studywolf.wordpress.com/" rel="external nofollow ugc" class="url">travisdewolf</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-2176">March 10, 2017 at 2:00 pm</a>		</div>

		<p>Don’t think you’re missing anything, really. You can do what you mention (and many do), but it involves editing your control policy online, and you’ll need to go in and start it up again if the environment changes. What the optimal policy is depends on what control algorithm you’re running, SARSA simply adapts to the control policy that is actually being used, as opposed to assuming that the optimal control policy will be used.</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=2176#respond" data-commentid="2176" data-postid="1043" data-belowelement="div-comment-2176" data-respondelement="respond" data-replyto="Reply to travisdewolf" aria-label="Reply to travisdewolf">Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment even thread-even depth-1 parent highlander-comment" id="comment-2182">
				<div id="div-comment-2182" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/photo.jpg" class="avatar avatar-32" height="32" width="32">			<cite class="fn"><a href="https://plus.google.com/+JonathanKowalski" rel="external nofollow ugc" class="url">Jonathan Kowalski</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-2182">March 13, 2017 at 7:47 pm</a>		</div>

		<p>Thanks for the post and thanks for all the replies.  It has been very helpful.  I am still having trouble and I hope I can ask this question very simply.</p>
<p>Q-Learning and SARSA start out the same way.   They greedly take an action on the first state.  </p>
<p>Q-learning takes the action that maximizes the q-values for the second state/action pair.  </p>
<p>SARSA takes the action that is determined by the policy.  </p>
<p>My question lies here:</p>
<p>Isn’t for both cases the policy defined by the q-values?  Q-learning is following a policy by taking the action that maximized the q-values for the second state/action pair.  SARSA’s policy is defined by the q-values we are learning and that policy would naturally be determined by the maximum state/action pairs.  </p>
<p>It seems like a chicken and the egg problem.  SARSA needs a policy to follow but that policy is determined by the q-values.  The q-values defines a policy by taking the maximum action for a state/action pair.</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=2182#respond" data-commentid="2182" data-postid="1043" data-belowelement="div-comment-2182" data-respondelement="respond" data-replyto="Reply to Jonathan Kowalski" aria-label="Reply to Jonathan Kowalski">Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-travisdewolf bypostauthor odd alt depth-2 highlander-comment" id="comment-2196">
				<div id="div-comment-2196" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/15812e89d7ec393690252d77ea02bf3d.png" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-15812e89d7ec393690252d77ea02bf3d-10">			<cite class="fn"><a href="https://studywolf.wordpress.com/" rel="external nofollow ugc" class="url">travisdewolf</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-2196">March 19, 2017 at 10:23 am</a>		</div>

		<p>Hello! I think I understand the confusion; there are two parts here.<br>
1) Q-learning and SARSA are different ways of _updating_ the Q-values. The control policy is entirely independent of how the q-values are updated. The difference between Q-learning and SARSA is that Q-learning makes an assumption about the control policy being used, and SARSA actually takes into account the behaviour of the control policy when updating q-values.<br>
2) For the chicken and egg part, i’m not sure that’s the best way to think about it. This is more a trial/error situation, where we can address the problem and improve performance by iterating. You start out with a poor model of the environment, act on that, see the results, improve the model, repeat. Our control policy rules are fully defined at the beginning, and behaviour only changes because the q-values change (as you say). Does that help?</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=2196#respond" data-commentid="2196" data-postid="1043" data-belowelement="div-comment-2196" data-respondelement="respond" data-replyto="Reply to travisdewolf" aria-label="Reply to travisdewolf">Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="pingback even thread-odd thread-alt depth-1 highlander-comment" id="comment-2239">
				<div id="div-comment-2239" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href="http://w2.xiupinzhe.com/69330.html" rel="external nofollow ugc" class="url">使用Keras和DDPG玩赛车游戏（自动驾驶） | 秀品折</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-2239">April 7, 2017 at 11:33 am</a>		</div>

		<p>[…] 区别辨析，直观易懂：Reinforcement Learning part 2: SARSA vs Q-learning […]</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=2239#respond" data-commentid="2239" data-postid="1043" data-belowelement="div-comment-2239" data-respondelement="respond" data-replyto="Reply to 使用Keras和DDPG玩赛车游戏（自动驾驶） | 秀品折" aria-label="Reply to 使用Keras和DDPG玩赛车游戏（自动驾驶） | 秀品折">Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt thread-even depth-1 parent highlander-comment" id="comment-2372">
				<div id="div-comment-2372" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/337f2673816a1df6bc0a959cee94c189" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-337f2673816a1df6bc0a959cee94c189-0">			<cite class="fn">Srh</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-2372">July 18, 2017 at 2:12 pm</a>		</div>

		<p>Hi, </p>
<p>Thanks for your good and informative post. I have a small question: Let’s assume that wusing past data of the system we were able to calculate T, R , and P and by solving an MDP we got the optimal policy in each state  as a result.</p>
<p>Now, assume that the environment is changing slowly and the optimal policy might not remain optimal anymore. Hence, I am looking for an algorithm to update Q value, and policy online. However, I do not want to act randomly because I know that I am already close to optimal policy and also acting randomly might makes problem for me. </p>
<p>So in your opinion, which one of these two algorithms ( or maybe sth else) can be better to update the current near optimal policy?</p>
<p>Thanks,</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=2372#respond" data-commentid="2372" data-postid="1043" data-belowelement="div-comment-2372" data-respondelement="respond" data-replyto="Reply to Srh" aria-label="Reply to Srh">Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-travisdewolf bypostauthor even depth-2 highlander-comment" id="comment-2889">
				<div id="div-comment-2889" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/15812e89d7ec393690252d77ea02bf3d.png" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-15812e89d7ec393690252d77ea02bf3d-11">			<cite class="fn"><a href="https://studywolf.wordpress.com/" rel="external nofollow ugc" class="url">travisdewolf</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-2889">December 1, 2017 at 10:39 pm</a>		</div>

		<p>Assuming that the near optimal policy you’re using is a function of learned or calculated Q-values…Starting from an already learned policy I think you would want to use SARSA, and you should just be able to take out the exploratory noise from the system. This will mean that the controller follows your specified near optimal controller until things behave unexpectedly. At which point it will update the Q-values, and your system will search around a bit to find another controller. Without the exploratory noise though you will miss out on searching for a local minima as the environment updates.</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=2889#respond" data-commentid="2889" data-postid="1043" data-belowelement="div-comment-2889" data-respondelement="respond" data-replyto="Reply to travisdewolf" aria-label="Reply to travisdewolf">Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="comment odd alt thread-odd thread-alt depth-1 parent highlander-comment" id="comment-2653">
				<div id="div-comment-2653" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/e21c363c93a938b9a266f781a5915c05" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-e21c363c93a938b9a266f781a5915c05-0">			<cite class="fn">Guang</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-2653">October 3, 2017 at 10:34 pm</a>		</div>

		<p>Hi,<br>
Great explanation about the difference!</p>
<p> I just have one question about Q-learning implementation. There are two kind of Q-learning algorithm. The older version, which is what you did in “cliff_Q.py” line 50, selecting the next action before update Q-value. Another one is just update the value and select next action in next iteration. I just want to know, is there any difference between those two versions, which one is correct? Thanks.</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=2653#respond" data-commentid="2653" data-postid="1043" data-belowelement="div-comment-2653" data-respondelement="respond" data-replyto="Reply to Guang" aria-label="Reply to Guang">Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-travisdewolf bypostauthor even depth-2 highlander-comment" id="comment-2670">
				<div id="div-comment-2670" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/15812e89d7ec393690252d77ea02bf3d.png" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-15812e89d7ec393690252d77ea02bf3d-12">			<cite class="fn"><a href="https://studywolf.wordpress.com/" rel="external nofollow ugc" class="url">travisdewolf</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-2670">October 5, 2017 at 9:31 am</a>		</div>

		<p>Hi Guang,</p>
<p>good question! So, for Q-learning, you’re updating the Q-values of the <em>previous</em> state, and then selecting an action for the <em>current</em> state, so you can choose an action and update or update and then choose an action, the update to the last state’s Q-values won’t have any effect on the action chosen in the current state.<br>
Cheers,</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=2670#respond" data-commentid="2670" data-postid="1043" data-belowelement="div-comment-2670" data-respondelement="respond" data-replyto="Reply to travisdewolf" aria-label="Reply to travisdewolf">Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		<li class="pingback odd alt thread-even depth-1 highlander-comment" id="comment-3710">
				<div id="div-comment-3710" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href="http://tianqincai.com/wordpress/index.php/2018/05/01/hello-world/" rel="external nofollow ugc" class="url">机器学习之Robocode – 上篇 – Cai Tianqin</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-3710">December 14, 2018 at 7:07 pm</a>		</div>

		<p>[…] 这里有一篇比课件上更好的解释：https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/ […]</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=3710#respond" data-commentid="3710" data-postid="1043" data-belowelement="div-comment-3710" data-respondelement="respond" data-replyto="Reply to 机器学习之Robocode – 上篇 – Cai Tianqin" aria-label="Reply to 机器学习之Robocode – 上篇 – Cai Tianqin">Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="pingback even thread-odd thread-alt depth-1 highlander-comment" id="comment-5825">
				<div id="div-comment-5825" class="comment-body">
				<div class="comment-author vcard">
						<cite class="fn"><a href="https://data-science-austria.at/2019/11/03/this-is-how-reinforcement-learning-works/" rel="external nofollow ugc" class="url">This Is How Reinforcement Learning Works – Data Science Austria</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-5825">November 2, 2019 at 10:51 pm</a>		</div>

		<p>[…] where the agent interacts with the environment and updates the policy based on actions taken. This post will give additional interesting insights about model-free […]</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=5825#respond" data-commentid="5825" data-postid="1043" data-belowelement="div-comment-5825" data-respondelement="respond" data-replyto="Reply to This Is How Reinforcement Learning Works – Data Science Austria" aria-label="Reply to This Is How Reinforcement Learning Works – Data Science Austria">Reply</a></div>
				</div>
				</li><!-- #comment-## -->
		<li class="comment odd alt thread-even depth-1 parent highlander-comment" id="comment-5981">
				<div id="div-comment-5981" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/c47511813dafd5f75c995ef551170c8f" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-c47511813dafd5f75c995ef551170c8f-0">			<cite class="fn">Bharath Kinnal</cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-5981">March 4, 2020 at 1:39 am</a>		</div>

		<p>So if I got this correctly, the difference between Q-learning and SARSA is that SARSA will try to expand the tree of possible states (s’) as much as possible, in order to get a solution from a wider state space, while Q-learning tries to expand the search tree in a more minimal fashion, and relying more on heuristics to find a good solution.</p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=5981#respond" data-commentid="5981" data-postid="1043" data-belowelement="div-comment-5981" data-respondelement="respond" data-replyto="Reply to Bharath Kinnal" aria-label="Reply to Bharath Kinnal">Reply</a></div>
				</div>
				<ul class="children">
		<li class="comment byuser comment-author-travisdewolf bypostauthor even depth-2 highlander-comment" id="comment-6000">
				<div id="div-comment-6000" class="comment-body">
				<div class="comment-author vcard">
			<img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/15812e89d7ec393690252d77ea02bf3d.png" class="avatar avatar-32 grav-hashed grav-hijack" height="32" width="32" id="grav-15812e89d7ec393690252d77ea02bf3d-13">			<cite class="fn"><a href="https://studywolf.wordpress.com/" rel="external nofollow ugc" class="url">travisdewolf</a></cite> <span class="says">says:</span>		</div>
		
		<div class="comment-meta commentmetadata">
			<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-6000">March 11, 2020 at 3:14 pm</a>		</div>

		<p>Hmmm, i don’t think that’s quite right. SARSA will update the expected reward of each state based on the actual control policy (which may be sub-optimal) where Q-learning updates the expected reward of each state assuming that the controller will always pick the optimal action. So SARSA will account for the chance that the controller sometimes unexpectedly jumps off the ledge, and you should stay away from the ledge, where Q-learning never will. Does that help? </p>

		<div class="reply"><a rel="nofollow" class="comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/?replytocom=6000#respond" data-commentid="6000" data-postid="1043" data-belowelement="div-comment-6000" data-respondelement="respond" data-replyto="Reply to travisdewolf" aria-label="Reply to travisdewolf">Reply</a></div>
				</div>
				</li><!-- #comment-## -->
</ul><!-- .children -->
</li><!-- #comment-## -->
		</ol>

		
	
	
		<div id="respond" class="comment-respond js">
		<h3 id="reply-title" class="comment-reply-title">Leave a Reply <small><a rel="nofollow" id="cancel-comment-reply-link" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#respond" style="display:none;">Cancel reply</a></small></h3><form action="https://studywolf.wordpress.com/wp-comments-post.php" method="post" id="commentform" class="comment-form"><input type="hidden" id="highlander_comment_nonce" name="highlander_comment_nonce" value="37e6b05a46"><input type="hidden" name="_wp_http_referer" value="/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/">
<input type="hidden" name="hc_post_as" id="hc_post_as" value="guest">

<div class="comment-form-field comment-textarea">
	
	<div id="comment-form-comment"><textarea aria-hidden="true" tabindex="-1" style="position: absolute; inset: -999px auto auto 0px; border: 0px; padding: 0px; box-sizing: content-box; overflow-wrap: break-word; overflow: hidden; transition: none 0s ease 0s; height: 0px !important; min-height: 0px !important; font-family: Arial, Helvetica, Tahoma, Verdana, sans-serif; font-size: 14px; font-weight: 400; font-style: normal; letter-spacing: 0px; text-transform: none; text-decoration: none solid rgba(0, 0, 0, 0.7); word-spacing: 0px; text-indent: 0px; line-height: normal; width: 578px;" class="autosizejs "></textarea><textarea id="comment" name="comment" title="Enter your comment here..." placeholder="Enter your comment here..." style="height: 36px; overflow: hidden; overflow-wrap: break-word; resize: none;"></textarea></div>
</div>

<div id="comment-form-identity" style="display: none;">
	<div id="comment-form-nascar">
		<p>Fill in your details below or click an icon to log in:</p>
		<ul>
			<li class="selected" style="display:none;">
				<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-form-guest" id="postas-guest" class="nascar-signin-link" title="Login via Guest">
									</a>
			</li>
			<li>
				<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-form-load-service:WordPress.com" id="postas-wordpress" class="nascar-signin-link" title="Login via WordPress.com">
					<svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#0087be" d="M12.158 12.786l-2.698 7.84c.806.236 1.657.365 2.54.365 1.047 0 2.05-.18 2.986-.51-.024-.037-.046-.078-.065-.123l-2.762-7.57zM3.008 12c0 3.56 2.07 6.634 5.068 8.092L3.788 8.342c-.5 1.117-.78 2.354-.78 3.658zm15.06-.454c0-1.112-.398-1.88-.74-2.48-.456-.74-.883-1.368-.883-2.11 0-.825.627-1.595 1.51-1.595.04 0 .078.006.116.008-1.598-1.464-3.73-2.36-6.07-2.36-3.14 0-5.904 1.613-7.512 4.053.21.008.41.012.58.012.94 0 2.395-.114 2.395-.114.484-.028.54.684.057.74 0 0-.487.058-1.03.086l3.275 9.74 1.968-5.902-1.4-3.838c-.485-.028-.944-.085-.944-.085-.486-.03-.43-.77.056-.742 0 0 1.484.114 2.368.114.94 0 2.397-.114 2.397-.114.486-.028.543.684.058.74 0 0-.488.058-1.03.086l3.25 9.665.897-2.997c.456-1.17.684-2.137.684-2.907zm1.82-3.86c.04.286.06.593.06.924 0 .912-.17 1.938-.683 3.22l-2.746 7.94c2.672-1.558 4.47-4.454 4.47-7.77 0-1.564-.4-3.033-1.1-4.314zM12 22C6.486 22 2 17.514 2 12S6.486 2 12 2s10 4.486 10 10-4.486 10-10 10z"></path></g></svg>				</a>
			</li>
			<li>
			<iframe id="googleplus-sign-in" name="googleplus-sign-in" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/saved_resource(11).html" width="24" height="24" scrolling="no" allowtransparency="true" seamless="seamless" frameborder="0"></iframe>
			</li>
			<li>
				<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-form-load-service:Twitter" id="postas-twitter" class="nascar-signin-link" title="Login via Twitter">
					<svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#1DA1F2" d="M22.23 5.924c-.736.326-1.527.547-2.357.646.847-.508 1.498-1.312 1.804-2.27-.793.47-1.67.812-2.606.996C18.325 4.498 17.258 4 16.078 4c-2.266 0-4.103 1.837-4.103 4.103 0 .322.036.635.106.935-3.41-.17-6.433-1.804-8.457-4.287-.353.607-.556 1.312-.556 2.064 0 1.424.724 2.68 1.825 3.415-.673-.022-1.305-.207-1.86-.514v.052c0 1.988 1.415 3.647 3.293 4.023-.344.095-.707.145-1.08.145-.265 0-.522-.026-.773-.074.522 1.63 2.038 2.817 3.833 2.85-1.404 1.1-3.174 1.757-5.096 1.757-.332 0-.66-.02-.98-.057 1.816 1.164 3.973 1.843 6.29 1.843 7.547 0 11.675-6.252 11.675-11.675 0-.178-.004-.355-.012-.53.802-.578 1.497-1.3 2.047-2.124z"></path></g></svg>				</a>
			</li>
			<li>
				<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#comment-form-load-service:Facebook" id="postas-facebook" class="nascar-signin-link" title="Login via Facebook">
					<svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#3B5998" d="M20.007 3H3.993C3.445 3 3 3.445 3 3.993v16.013c0 .55.445.994.993.994h8.62v-6.97H10.27V11.31h2.346V9.31c0-2.325 1.42-3.59 3.494-3.59.993 0 1.847.073 2.096.106v2.43h-1.438c-1.128 0-1.346.537-1.346 1.324v1.734h2.69l-.35 2.717h-2.34V21h4.587c.548 0 .993-.445.993-.993V3.993c0-.548-.445-.993-.993-.993z"></path></g></svg>				</a>
			</li>
		</ul>
	</div>

	<div id="comment-form-guest" class="comment-form-service selected">
		<div class="comment-form-padder">
			<div class="comment-form-avatar">
<a href="https://gravatar.com/site/signup/" target="_blank">				<img src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/ad516503a11cd5ca435acc9bb6523536" alt="Gravatar" width="25" class="no-grav grav-hashed grav-hijack" id="grav-ad516503a11cd5ca435acc9bb6523536-0">
</a>			</div>

				<div class="comment-form-fields">
				<div class="comment-form-field comment-form-email">
					<label for="email">Email <span class="required">(required)</span> <span class="nopublish">(Address never made public)</span></label>
					<div class="comment-form-input"><input id="email" name="email" type="email" value=""></div>
				</div>
				<div class="comment-form-field comment-form-author">
					<label for="author">Name <span class="required">(required)</span></label>
					<div class="comment-form-input"><input id="author" name="author" type="text" value=""></div>
				</div>
				<div class="comment-form-field comment-form-url">
					<label for="url">Website</label>
					<div class="comment-form-input"><input id="url" name="url" type="url" value=""></div>
				</div>
			</div>
			
		</div>
	</div>

	<div id="comment-form-wordpress" class="comment-form-service">
		<div class="comment-form-padder">
			<div class="comment-form-avatar">
				<img src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/ad516503a11cd5ca435acc9bb6523536" alt="WordPress.com Logo" width="25" class="no-grav grav-hashed grav-hijack" id="grav-ad516503a11cd5ca435acc9bb6523536-1">
			</div>

				<div class="comment-form-fields">
				<input type="hidden" name="wp_avatar" id="wordpress-avatar" class="comment-meta-wordpress" value="">
				<input type="hidden" name="wp_user_id" id="wordpress-user_id" class="comment-meta-wordpress" value="">
				<input type="hidden" name="wp_access_token" id="wordpress-access_token" class="comment-meta-wordpress" value="">
						<p class="comment-form-posting-as pa-wordpress">
			<strong></strong>
			You are commenting using your WordPress.com account.			<span class="comment-form-log-out">
				(&nbsp;<a href="javascript:HighlanderComments.doExternalLogout( &#39;wordpress&#39; );">Log&nbsp;Out</a>&nbsp;/&nbsp;
				<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#" onclick="javascript:HighlanderComments.switchAccount();return false;">Change</a>&nbsp;)
			</span>
			<span class="pa-icon"><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#0087be" d="M12.158 12.786l-2.698 7.84c.806.236 1.657.365 2.54.365 1.047 0 2.05-.18 2.986-.51-.024-.037-.046-.078-.065-.123l-2.762-7.57zM3.008 12c0 3.56 2.07 6.634 5.068 8.092L3.788 8.342c-.5 1.117-.78 2.354-.78 3.658zm15.06-.454c0-1.112-.398-1.88-.74-2.48-.456-.74-.883-1.368-.883-2.11 0-.825.627-1.595 1.51-1.595.04 0 .078.006.116.008-1.598-1.464-3.73-2.36-6.07-2.36-3.14 0-5.904 1.613-7.512 4.053.21.008.41.012.58.012.94 0 2.395-.114 2.395-.114.484-.028.54.684.057.74 0 0-.487.058-1.03.086l3.275 9.74 1.968-5.902-1.4-3.838c-.485-.028-.944-.085-.944-.085-.486-.03-.43-.77.056-.742 0 0 1.484.114 2.368.114.94 0 2.397-.114 2.397-.114.486-.028.543.684.058.74 0 0-.488.058-1.03.086l3.25 9.665.897-2.997c.456-1.17.684-2.137.684-2.907zm1.82-3.86c.04.286.06.593.06.924 0 .912-.17 1.938-.683 3.22l-2.746 7.94c2.672-1.558 4.47-4.454 4.47-7.77 0-1.564-.4-3.033-1.1-4.314zM12 22C6.486 22 2 17.514 2 12S6.486 2 12 2s10 4.486 10 10-4.486 10-10 10z"></path></g></svg></span>
		</p>
					</div>
	
		</div>
	</div>

	<div id="comment-form-googleplus" class="comment-form-service">
		<div class="comment-form-padder">
			<div class="comment-form-avatar">
				<img src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/ad516503a11cd5ca435acc9bb6523536" alt="Google photo" width="25" class="no-grav grav-hashed grav-hijack" id="grav-ad516503a11cd5ca435acc9bb6523536-2">
			</div>

				<div class="comment-form-fields">
				<input type="hidden" name="googleplus_avatar" id="googleplus-avatar" class="comment-meta-googleplus" value="">
				<input type="hidden" name="googleplus_user_id" id="googleplus-user_id" class="comment-meta-googleplus" value="">
				<input type="hidden" name="googleplus_access_token" id="googleplus-access_token" class="comment-meta-googleplus" value="">
						<p class="comment-form-posting-as pa-googleplus">
			<strong></strong>
			You are commenting using your Google account.			<span class="comment-form-log-out">
				(&nbsp;<a href="javascript:HighlanderComments.doExternalLogout( &#39;googleplus&#39; );">Log&nbsp;Out</a>&nbsp;/&nbsp;
				<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#" onclick="javascript:HighlanderComments.switchAccount();return false;">Change</a>&nbsp;)
			</span>
			<span class="pa-icon"><svg xmlns="http://www.w3.org/2000/svg" role="presentation" x="0px" y="0px" viewBox="0 0 60 60"><path fill="#519bf7" d="M56.3,30c0,-1.6 -0.2,-3.4 -0.6,-5h-3.1H42.2H30v10.6h14.8C44,39.3 42,42 39.1,43.9l8.8,6.8C53,46 56.3,39 56.3,30z"></path><path fill="#3db366" d="M30,57.5c6.7,0 13.1,-2.4 17.9,-6.8l-8.8,-6.8c-2.5,1.6 -5.6,2.4 -9.1,2.4c-7.2,0 -13.3,-4.7 -15.4,-11.2l-9.3,7.1C9.8,51.3 19.1,57.5 30,57.5z"></path><path fill="#fdc600" d="M5.3,42.2l9.3,-7.1c-0.5,-1.6 -0.8,-3.3 -0.8,-5.1s0.3,-3.5 0.8,-5.1l-9.3,-7.1C3.5,21.5 2.5,25.6 2.5,30S3.5,38.5 5.3,42.2z"></path><path fill="#f15b44" d="M40.1,17.4l8,-8C43.3,5.1 37,2.5 30,2.5C19.1,2.5 9.8,8.7 5.3,17.8l9.3,7.1c2.1,-6.5 8.2,-11.1 15.4,-11.1C33.9,13.7 37.4,15.1 40.1,17.4z"></path></svg></span>
		</p>
					</div>
	
		</div>
	</div>

	<div id="comment-form-twitter" class="comment-form-service">
		<div class="comment-form-padder">
			<div class="comment-form-avatar">
				<img src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/ad516503a11cd5ca435acc9bb6523536" alt="Twitter picture" width="25" class="no-grav grav-hashed grav-hijack" id="grav-ad516503a11cd5ca435acc9bb6523536-3">
			</div>

				<div class="comment-form-fields">
				<input type="hidden" name="twitter_avatar" id="twitter-avatar" class="comment-meta-twitter" value="">
				<input type="hidden" name="twitter_user_id" id="twitter-user_id" class="comment-meta-twitter" value="">
				<input type="hidden" name="twitter_access_token" id="twitter-access_token" class="comment-meta-twitter" value="">
						<p class="comment-form-posting-as pa-twitter">
			<strong></strong>
			You are commenting using your Twitter account.			<span class="comment-form-log-out">
				(&nbsp;<a href="javascript:HighlanderComments.doExternalLogout( &#39;twitter&#39; );">Log&nbsp;Out</a>&nbsp;/&nbsp;
				<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#" onclick="javascript:HighlanderComments.switchAccount();return false;">Change</a>&nbsp;)
			</span>
			<span class="pa-icon"><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#1DA1F2" d="M22.23 5.924c-.736.326-1.527.547-2.357.646.847-.508 1.498-1.312 1.804-2.27-.793.47-1.67.812-2.606.996C18.325 4.498 17.258 4 16.078 4c-2.266 0-4.103 1.837-4.103 4.103 0 .322.036.635.106.935-3.41-.17-6.433-1.804-8.457-4.287-.353.607-.556 1.312-.556 2.064 0 1.424.724 2.68 1.825 3.415-.673-.022-1.305-.207-1.86-.514v.052c0 1.988 1.415 3.647 3.293 4.023-.344.095-.707.145-1.08.145-.265 0-.522-.026-.773-.074.522 1.63 2.038 2.817 3.833 2.85-1.404 1.1-3.174 1.757-5.096 1.757-.332 0-.66-.02-.98-.057 1.816 1.164 3.973 1.843 6.29 1.843 7.547 0 11.675-6.252 11.675-11.675 0-.178-.004-.355-.012-.53.802-.578 1.497-1.3 2.047-2.124z"></path></g></svg></span>
		</p>
					</div>
	
		</div>
	</div>

	<div id="comment-form-facebook" class="comment-form-service">
		<div class="comment-form-padder">
			<div class="comment-form-avatar">
				<img src="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/" alt="Facebook photo" width="25" class="no-grav">
			</div>

				<div class="comment-form-fields">
				<input type="hidden" name="fb_avatar" id="facebook-avatar" class="comment-meta-facebook" value="">
				<input type="hidden" name="fb_user_id" id="facebook-user_id" class="comment-meta-facebook" value="">
				<input type="hidden" name="fb_access_token" id="facebook-access_token" class="comment-meta-facebook" value="">
						<p class="comment-form-posting-as pa-facebook">
			<strong></strong>
			You are commenting using your Facebook account.			<span class="comment-form-log-out">
				(&nbsp;<a href="javascript:HighlanderComments.doExternalLogout( &#39;facebook&#39; );">Log&nbsp;Out</a>&nbsp;/&nbsp;
				<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#" onclick="javascript:HighlanderComments.switchAccount();return false;">Change</a>&nbsp;)
			</span>
			<span class="pa-icon"><svg xmlns="http://www.w3.org/2000/svg" role="presentation" viewBox="0 0 24 24"><rect x="0" fill="none" width="24" height="24"></rect><g><path fill="#3B5998" d="M20.007 3H3.993C3.445 3 3 3.445 3 3.993v16.013c0 .55.445.994.993.994h8.62v-6.97H10.27V11.31h2.346V9.31c0-2.325 1.42-3.59 3.494-3.59.993 0 1.847.073 2.096.106v2.43h-1.438c-1.128 0-1.346.537-1.346 1.324v1.734h2.69l-.35 2.717h-2.34V21h4.587c.548 0 .993-.445.993-.993V3.993c0-.548-.445-.993-.993-.993z"></path></g></svg></span>
		</p>
					</div>
	
		</div>
	</div>


	<div id="comment-form-load-service" class="comment-form-service">
		<div class="comment-form-posting-as-cancel"><a href="javascript:HighlanderComments.cancelExternalWindow();">Cancel</a></div>
		<p>Connecting to %s</p>
	</div>

</div>

<script type="text/javascript">
var highlander_expando_javascript = function () {

	function hide( sel ) {
		var el = document.querySelector( sel );
		if ( el ) {
			el.style.setProperty( 'display', 'none' );
		}
	}

	function show( sel ) {
		var el = document.querySelector( sel );
		if ( el ) {
			el.style.removeProperty( 'display' );
		}
	}

	var input = document.createElement( 'input' );
	var comment = document.querySelector( '#comment' );

	if ( input && comment && 'placeholder' in input ) {
		var label = document.querySelector( '.comment-textarea label' );
		if ( label ) {
			var text = label.textContent;
			label.parentNode.removeChild( label );
			comment.setAttribute( 'placeholder', text );
		}
	}

	// Expando Mode: start small, then auto-resize on first click + text length
	hide( '#comment-form-identity' );
	hide( '#comment-form-subscribe' );
	hide( '#commentform .form-submit' );

	if ( comment ) {
		comment.style.height = '10px';

		var handler = function () {
			comment.style.height = HighlanderComments.initialHeight + 'px';
			show( '#comment-form-identity' );
			show( '#comment-form-subscribe' );
			show( '#commentform .form-submit' );
			HighlanderComments.resizeCallback();

			comment.removeEventListener( 'focus', handler );
		};

		comment.addEventListener( 'focus', handler );
	}
}

if ( document.readyState !== 'loading' ) {
	highlander_expando_javascript();
} else {
	if ( typeof window.jQuery === 'function' ) {
		// Use jQuery's `ready` if available.
		// This solves some scheduling issues between this script and the main highlander script.
		jQuery( document ).ready( highlander_expando_javascript );
	} else {
		// If not available, add a vanilla event listener.
		document.addEventListener( 'DOMContentLoaded', highlander_expando_javascript );
	}
}

</script>

<div id="comment-form-subscribe" style="display: none;">
	<p class="comment-subscription-form"><input type="checkbox" name="subscribe" id="subscribe" value="subscribe" style="width: auto;"> <label class="subscribe-label" id="subscribe-label" for="subscribe" style="display: inline;">Notify me of new comments via email.</label></p><p class="post-subscription-form"><input type="checkbox" name="subscribe_blog" id="subscribe_blog" value="subscribe" style="width: auto;"> <label class="subscribe-label" id="subscribe-blog-label" for="subscribe_blog" style="display: inline;">Notify me of new posts via email.</label></p></div>




<p class="form-submit wp-block-button" style="display: none;"><input name="submit" type="submit" id="comment-submit" class="submit wp-block-button__link" value="Post Comment"> <input type="hidden" name="comment_post_ID" value="1043" id="comment_post_ID">
<input type="hidden" name="comment_parent" id="comment_parent" value="0">
</p><p style="display: none;"><input type="hidden" id="akismet_comment_nonce" name="akismet_comment_nonce" value="9e5ef3552c"></p>
<input type="hidden" name="genseq" value="1630658298">
<textarea name="ak_hp_textarea" cols="45" rows="8" maxlength="100" style="display: none !important;"></textarea><input type="hidden" id="ak_js" name="ak_js" value="1630658299297"></form>	</div><!-- #respond -->
	<div style="clear: both"></div>
</div><!-- #comments -->			</div><!-- #contents -->

	<div class="navigation">
		<div class="nav-previous"><a href="https://studywolf.wordpress.com/2013/05/21/wrapping-maplesim-c-code-for-python/" rel="prev"><span class="meta-nav">←</span> Previous post</a></div>
		<div class="nav-next"><a href="https://studywolf.wordpress.com/2013/08/21/robot-control-forward-transformation-matrices/" rel="next">Next post <span class="meta-nav">→</span></a></div>
	</div>


<div id="widgets">
		<div class="widget-area">
		<aside id="search-2" class="widget widget_search"><form role="search" method="get" id="searchform" class="searchform" action="https://studywolf.wordpress.com/">
				<div>
					<label class="screen-reader-text" for="s">Search for:</label>
					<input type="text" value="" name="s" id="s">
					<input type="submit" id="searchsubmit" value="Search">
				</div>
			</form></aside>
		<aside id="recent-posts-2" class="widget widget_recent_entries">
		<h3 class="widget-title">Recent Posts</h3>
		<ul>
											<li>
					<a href="https://studywolf.wordpress.com/2021/08/27/abr-control-supports-mujoco/">ABR Control supports&nbsp;Mujoco!</a>
									</li>
											<li>
					<a href="https://studywolf.wordpress.com/2020/12/01/adaptive-neurorobotics-using-nengo-and-the-loihi/">Workshop talk – Adaptive neurorobotics using Nengo and the&nbsp;Loihi</a>
									</li>
											<li>
					<a href="https://studywolf.wordpress.com/2020/03/22/building-models-in-mujoco/">Building models in&nbsp;Mujoco</a>
									</li>
											<li>
					<a href="https://studywolf.wordpress.com/2020/03/13/converting-your-keras-model-into-a-spiking-neural-network-using-nengo-dl/">Converting your Keras model into a spiking neural&nbsp;network</a>
									</li>
											<li>
					<a href="https://studywolf.wordpress.com/2018/12/03/force-control-of-task-space-orientation/">Force control of task-space&nbsp;orientation</a>
									</li>
					</ul>

		</aside><aside id="archives-2" class="widget widget_archive"><h3 class="widget-title">Archives</h3>		<label class="screen-reader-text" for="archives-dropdown-2">Archives</label>
		<select id="archives-dropdown-2" name="archive-dropdown">
			
			<option value="">Select Month</option>
				<option value="https://studywolf.wordpress.com/2021/08/"> August 2021 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2020/12/"> December 2020 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2020/03/"> March 2020 &nbsp;(2)</option>
	<option value="https://studywolf.wordpress.com/2018/12/"> December 2018 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2018/06/"> June 2018 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2018/01/"> January 2018 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2017/11/"> November 2017 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2017/09/"> September 2017 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2017/07/"> July 2017 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2017/06/"> June 2017 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2017/05/"> May 2017 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2017/04/"> April 2017 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2016/11/"> November 2016 &nbsp;(2)</option>
	<option value="https://studywolf.wordpress.com/2016/08/"> August 2016 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2016/05/"> May 2016 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2016/04/"> April 2016 &nbsp;(2)</option>
	<option value="https://studywolf.wordpress.com/2016/03/"> March 2016 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2016/02/"> February 2016 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2015/11/"> November 2015 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2015/10/"> October 2015 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2015/06/"> June 2015 &nbsp;(3)</option>
	<option value="https://studywolf.wordpress.com/2015/04/"> April 2015 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2015/03/"> March 2015 &nbsp;(3)</option>
	<option value="https://studywolf.wordpress.com/2014/07/"> July 2014 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2014/03/"> March 2014 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2013/12/"> December 2013 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2013/11/"> November 2013 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2013/10/"> October 2013 &nbsp;(2)</option>
	<option value="https://studywolf.wordpress.com/2013/09/"> September 2013 &nbsp;(5)</option>
	<option value="https://studywolf.wordpress.com/2013/08/"> August 2013 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2013/07/"> July 2013 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2013/05/"> May 2013 &nbsp;(2)</option>
	<option value="https://studywolf.wordpress.com/2013/04/"> April 2013 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2013/03/"> March 2013 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2013/02/"> February 2013 &nbsp;(3)</option>
	<option value="https://studywolf.wordpress.com/2013/01/"> January 2013 &nbsp;(2)</option>
	<option value="https://studywolf.wordpress.com/2012/12/"> December 2012 &nbsp;(1)</option>
	<option value="https://studywolf.wordpress.com/2012/11/"> November 2012 &nbsp;(2)</option>
	<option value="https://studywolf.wordpress.com/2012/10/"> October 2012 &nbsp;(3)</option>
	<option value="https://studywolf.wordpress.com/2012/09/"> September 2012 &nbsp;(5)</option>

		</select>

<script type="text/javascript">
/* <![CDATA[ */
(function() {
	var dropdown = document.getElementById( "archives-dropdown-2" );
	function onSelectChange() {
		if ( dropdown.options[ dropdown.selectedIndex ].value !== '' ) {
			document.location.href = this.options[ this.selectedIndex ].value;
		}
	}
	dropdown.onchange = onSelectChange;
})();
/* ]]> */
</script>
			</aside><aside id="categories-2" class="widget widget_categories"><h3 class="widget-title">Categories</h3>
			<ul>
					<li class="cat-item cat-item-13038239"><a href="https://studywolf.wordpress.com/category/cython/">Cython</a> (2)
</li>
	<li class="cat-item cat-item-264"><a href="https://studywolf.wordpress.com/category/learning/">Learning</a> (10)
<ul class="children">
	<li class="cat-item cat-item-1131271"><a href="https://studywolf.wordpress.com/category/learning/deep-learning/">Deep Learning</a> (3)
</li>
	<li class="cat-item cat-item-101572"><a href="https://studywolf.wordpress.com/category/learning/reinforcement-learning/">Reinforcement Learning</a> (5)
</li>
</ul>
</li>
	<li class="cat-item cat-item-610"><a href="https://studywolf.wordpress.com/category/linux/">Linux</a> (2)
</li>
	<li class="cat-item cat-item-2813"><a href="https://studywolf.wordpress.com/category/math/">math</a> (16)
<ul class="children">
	<li class="cat-item cat-item-69043"><a href="https://studywolf.wordpress.com/category/math/linear-algebra/">linear algebra</a> (8)
</li>
	<li class="cat-item cat-item-2273773"><a href="https://studywolf.wordpress.com/category/math/probability-math/">probability</a> (1)
</li>
</ul>
</li>
	<li class="cat-item cat-item-1671714"><a href="https://studywolf.wordpress.com/category/motor-control/">motor control</a> (33)
</li>
	<li class="cat-item cat-item-228584552"><a href="https://studywolf.wordpress.com/category/neuromorphic-hardware/">neuromorphic hardware</a> (1)
</li>
	<li class="cat-item cat-item-27026"><a href="https://studywolf.wordpress.com/category/neuroscience/">neuroscience</a> (9)
<ul class="children">
	<li class="cat-item cat-item-3142498"><a href="https://studywolf.wordpress.com/category/neuroscience/basal-ganglia/">basal ganglia</a> (2)
</li>
</ul>
</li>
	<li class="cat-item cat-item-10451"><a href="https://studywolf.wordpress.com/category/probability/">probability</a> (2)
</li>
	<li class="cat-item cat-item-196"><a href="https://studywolf.wordpress.com/category/programming/">programming</a> (45)
<ul class="children">
	<li class="cat-item cat-item-2426"><a href="https://studywolf.wordpress.com/category/programming/c/">C++</a> (10)
	<ul class="children">
	<li class="cat-item cat-item-174530"><a href="https://studywolf.wordpress.com/category/programming/c/eigen/">Eigen</a> (1)
</li>
	</ul>
</li>
	<li class="cat-item cat-item-112776241"><a href="https://studywolf.wordpress.com/category/programming/cython-programming/">Cython</a> (7)
</li>
	<li class="cat-item cat-item-11008239"><a href="https://studywolf.wordpress.com/category/programming/nengo/">Nengo</a> (9)
</li>
	<li class="cat-item cat-item-832"><a href="https://studywolf.wordpress.com/category/programming/python/">Python</a> (35)
</li>
	<li class="cat-item cat-item-55890949"><a href="https://studywolf.wordpress.com/category/programming/vrep/">vrep</a> (5)
</li>
</ul>
</li>
	<li class="cat-item cat-item-13426"><a href="https://studywolf.wordpress.com/category/robotics/">Robotics</a> (32)
<ul class="children">
	<li class="cat-item cat-item-201135428"><a href="https://studywolf.wordpress.com/category/robotics/dynamic-movement-primitive/">Dynamic movement primitive</a> (4)
</li>
	<li class="cat-item cat-item-419515887"><a href="https://studywolf.wordpress.com/category/robotics/mujoco/">Mujoco</a> (3)
</li>
	<li class="cat-item cat-item-194149525"><a href="https://studywolf.wordpress.com/category/robotics/operational-space-control/">Operational space control</a> (18)
</li>
	<li class="cat-item cat-item-6739586"><a href="https://studywolf.wordpress.com/category/robotics/pid-control/">PID control</a> (7)
</li>
</ul>
</li>
			</ul>

			</aside><aside id="meta-2" class="widget widget_meta"><h3 class="widget-title">Meta</h3>
		<ul>
			<li><a href="https://wordpress.com/start?ref=wplogin">Register</a></li>			<li><a href="https://studywolf.wordpress.com/wp-login.php">Log in</a></li>
			<li><a href="https://studywolf.wordpress.com/feed/">Entries feed</a></li>
			<li><a href="https://studywolf.wordpress.com/comments/feed/">Comments feed</a></li>

			<li><a href="https://wordpress.com/" title="Powered by WordPress, state-of-the-art semantic personal publishing platform.">WordPress.com</a></li>
		</ul>

		</aside><aside id="tag_cloud-3" class="widget widget_tag_cloud"><h3 class="widget-title"></h3><div style="overflow: hidden;"><a href="https://studywolf.wordpress.com/category/neuroscience/basal-ganglia/" style="font-size: 103.97727272727%; padding: 1px; margin: 1px;" title="basal ganglia (2)">basal ganglia</a> <a href="https://studywolf.wordpress.com/category/programming/c/" style="font-size: 135.79545454545%; padding: 1px; margin: 1px;" title="C++ (10)">C++</a> <a href="https://studywolf.wordpress.com/tag/cython/" style="font-size: 103.97727272727%; padding: 1px; margin: 1px;" title="Cython (2)">Cython</a> <a href="https://studywolf.wordpress.com/category/learning/deep-learning/" style="font-size: 107.95454545455%; padding: 1px; margin: 1px;" title="Deep Learning (3)">Deep Learning</a> <a href="https://studywolf.wordpress.com/category/robotics/dynamic-movement-primitive/" style="font-size: 111.93181818182%; padding: 1px; margin: 1px;" title="Dynamic movement primitive (4)">Dynamic movement primitive</a> <a href="https://studywolf.wordpress.com/category/programming/c/eigen/" style="font-size: 100%; padding: 1px; margin: 1px;" title="Eigen (1)">Eigen</a> <a href="https://studywolf.wordpress.com/category/learning/" style="font-size: 135.79545454545%; padding: 1px; margin: 1px;" title="Learning (10)">Learning</a> <a href="https://studywolf.wordpress.com/category/math/linear-algebra/" style="font-size: 127.84090909091%; padding: 1px; margin: 1px;" title="linear algebra (8)">linear algebra</a> <a href="https://studywolf.wordpress.com/category/linux/" style="font-size: 103.97727272727%; padding: 1px; margin: 1px;" title="Linux (2)">Linux</a> <a href="https://studywolf.wordpress.com/category/math/" style="font-size: 159.65909090909%; padding: 1px; margin: 1px;" title="math (16)">math</a> <a href="https://studywolf.wordpress.com/category/motor-control/" style="font-size: 227.27272727273%; padding: 1px; margin: 1px;" title="motor control (33)">motor control</a> <a href="https://studywolf.wordpress.com/category/robotics/mujoco/" style="font-size: 107.95454545455%; padding: 1px; margin: 1px;" title="Mujoco (3)">Mujoco</a> <a href="https://studywolf.wordpress.com/category/programming/nengo/" style="font-size: 131.81818181818%; padding: 1px; margin: 1px;" title="Nengo (9)">Nengo</a> <a href="https://studywolf.wordpress.com/category/neuromorphic-hardware/" style="font-size: 100%; padding: 1px; margin: 1px;" title="neuromorphic hardware (1)">neuromorphic hardware</a> <a href="https://studywolf.wordpress.com/category/neuroscience/" style="font-size: 131.81818181818%; padding: 1px; margin: 1px;" title="neuroscience (9)">neuroscience</a> <a href="https://studywolf.wordpress.com/category/robotics/operational-space-control/" style="font-size: 167.61363636364%; padding: 1px; margin: 1px;" title="Operational space control (18)">Operational space control</a> <a href="https://studywolf.wordpress.com/category/robotics/pid-control/" style="font-size: 123.86363636364%; padding: 1px; margin: 1px;" title="PID control (7)">PID control</a> <a href="https://studywolf.wordpress.com/category/math/probability-math/" style="font-size: 100%; padding: 1px; margin: 1px;" title="probability (1)">probability</a> <a href="https://studywolf.wordpress.com/category/programming/" style="font-size: 275%; padding: 1px; margin: 1px;" title="programming (45)">programming</a> <a href="https://studywolf.wordpress.com/tag/python/" style="font-size: 235.22727272727%; padding: 1px; margin: 1px;" title="Python (35)">Python</a> <a href="https://studywolf.wordpress.com/category/learning/reinforcement-learning/" style="font-size: 115.90909090909%; padding: 1px; margin: 1px;" title="Reinforcement Learning (5)">Reinforcement Learning</a> <a href="https://studywolf.wordpress.com/category/robotics/" style="font-size: 223.29545454545%; padding: 1px; margin: 1px;" title="Robotics (32)">Robotics</a> <a href="https://studywolf.wordpress.com/category/programming/vrep/" style="font-size: 115.90909090909%; padding: 1px; margin: 1px;" title="vrep (5)">vrep</a> </div></aside>			<div id="atatags-286348-6131defa52aba"></div>
			
			<script>
				__ATA.cmd.push(function() {
					__ATA.initDynamicSlot({
						id: 'atatags-286348-6131defa52aba',
						location: 140,
						formFactor: '003',
						label: {
							text: 'Advertisements',
						},
						creative: {
							reportAd: {
								text: 'Report this ad',
							},
							privacySettings: {
								text: 'Privacy',
								onClick: function() { window.__tcfapi && window.__tcfapi( 'showUi' ); },
							}
						}
					});
				});
			</script>	</div><!-- #first .widget-area -->
	</div><!-- #widgets -->
	<div id="footer">
		<a href="https://wordpress.com/?ref=footer_blog" rel="nofollow">Blog at WordPress.com.</a><a href="https://wordpress.com/advertising-program-optout/" class="do-not-sell-link" rel="nofollow" style="margin-left: 0.5em;">Do Not Sell My Personal Information</a>
		
			</div>

</div>

<!--  -->
<script src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/gprofiles.js.téléchargé" id="grofiles-cards-js"></script>
<script id="wpgroho-js-extra">
var WPGroHo = {"my_hash":""};
</script>
<script crossorigin="anonymous" type="text/javascript" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/wpgroho.js.téléchargé"></script>

	<script>
		// Initialize and attach hovercards to all gravatars
		( function() {
			function init() {
				if ( typeof Gravatar === 'undefined' ) {
					return;
				}

				if ( typeof Gravatar.init !== 'function' ) {
					return;
				}

				Gravatar.profile_cb = function ( hash, id ) {
					WPGroHo.syncProfileData( hash, id );
				};

				Gravatar.my_hash = WPGroHo.my_hash;
				Gravatar.init( 'body', '#wp-admin-bar-my-account' );
			}

			if ( document.readyState !== 'loading' ) {
				init();
			} else {
				document.addEventListener( 'DOMContentLoaded', init );
			}
		} )();
	</script>

		<div style="display:none">
	<div class="grofile-hash-map-a64192cf9c1f5e91ffc83577918b610e">
	</div>
	<div class="grofile-hash-map-15812e89d7ec393690252d77ea02bf3d">
	</div>
	<div class="grofile-hash-map-b2f24b53caabf7ee37a2a2ab182f25a2">
	</div>
	<div class="grofile-hash-map-9ab56126aeee1cbe7218fc7c65a21807">
	</div>
	<div class="grofile-hash-map-602683f4544da7e81edab7ca3c0e49b6">
	</div>
	<div class="grofile-hash-map-bca24b3fc0c716ffaf8fd3f0138050c0">
	</div>
	<div class="grofile-hash-map-29a142f7a31ab6203692a702f37fdf11">
	</div>
	<div class="grofile-hash-map-b27b1ea94ea1c641fb621a21738ea631">
	</div>
	<div class="grofile-hash-map-0215713b62e3606c3d2ec991824eb754">
	</div>
	<div class="grofile-hash-map-77746485693c297f4ecd5ee4c3f2cc6e">
	</div>
	<div class="grofile-hash-map-b984dbfc6bf74dc63a0ebf044975059e">
	</div>
	<div class="grofile-hash-map-337f2673816a1df6bc0a959cee94c189">
	</div>
	<div class="grofile-hash-map-e21c363c93a938b9a266f781a5915c05">
	</div>
	<div class="grofile-hash-map-c47511813dafd5f75c995ef551170c8f">
	</div>
	</div>
<script id="highlander-comments-js-extra">
var HighlanderComments = {"loggingInText":"Logging In\u2026","submittingText":"Posting Comment\u2026","postCommentText":"Post Comment","connectingToText":"Connecting to %s","commentingAsText":"%1$s: You are commenting using your %2$s account.","logoutText":"Log Out","loginText":"Log In","connectURL":"https:\/\/studywolf.wordpress.com\/public.api\/connect\/?action=request","logoutURL":"https:\/\/studywolf.wordpress.com\/wp-login.php?action=logout&_wpnonce=b4c34a8444","homeURL":"https:\/\/studywolf.wordpress.com\/","postID":"1043","gravDefault":"identicon","enterACommentError":"Please enter a comment","enterEmailError":"Please enter your email address here","invalidEmailError":"Invalid email address","enterAuthorError":"Please enter your name here","gravatarFromEmail":"This picture will show whenever you leave a comment. Click to customize it.","logInToExternalAccount":"Log in to use details from one of these accounts.","change":"Change","changeAccount":"Change Account","comment_registration":"","userIsLoggedIn":"","isJetpack":"","text_direction":"ltr"};
</script>
<script crossorigin="anonymous" type="text/javascript" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/saved_resource(6)"></script>
		<!-- CCPA [start] -->
		<script type="text/javascript">
			( function () {

				var setupPrivacy = function() {

					// Minimal Mozilla Cookie library
					// https://developer.mozilla.org/en-US/docs/Web/API/Document/cookie/Simple_document.cookie_framework
					var cookieLib = window.cookieLib = {getItem:function(e){return e&&decodeURIComponent(document.cookie.replace(new RegExp("(?:(?:^|.*;)\\s*"+encodeURIComponent(e).replace(/[\-\.\+\*]/g,"\\$&")+"\\s*\\=\\s*([^;]*).*$)|^.*$"),"$1"))||null},setItem:function(e,o,n,t,r,i){if(!e||/^(?:expires|max\-age|path|domain|secure)$/i.test(e))return!1;var c="";if(n)switch(n.constructor){case Number:c=n===1/0?"; expires=Fri, 31 Dec 9999 23:59:59 GMT":"; max-age="+n;break;case String:c="; expires="+n;break;case Date:c="; expires="+n.toUTCString()}return"rootDomain"!==r&&".rootDomain"!==r||(r=(".rootDomain"===r?".":"")+document.location.hostname.split(".").slice(-2).join(".")),document.cookie=encodeURIComponent(e)+"="+encodeURIComponent(o)+c+(r?"; domain="+r:"")+(t?"; path="+t:"")+(i?"; secure":""),!0}};

					// Implement IAB USP API.
					window.__uspapi = function( command, version, callback ) {

						if ( typeof callback !== 'function' ) {
							return;
						}

						if ( command !== 'getUSPData' || version !== 1 ) {
							callback( null, false );
							return;
						}

						// Check for GPC.
						if ( navigator.globalPrivacyControl ) {
							callback( { version: 1, uspString: '1YYN' }, true );
							return;
						}

						// Check for cookie.
						var consent = cookieLib.getItem( 'usprivacy' );

						if ( null === consent ) {
							callback( null, false );
							return;
						}

						callback( { version: 1, uspString: consent }, true );
					};

					// Initialization.
					document.addEventListener( 'DOMContentLoaded', function() {

						var setDefaultOptInCookie = function() {
							var value = '1YNN';
							var domain = '.wordpress.com' === location.hostname.slice( -14 ) ? '.rootDomain' : location.hostname;
							cookieLib.setItem( 'usprivacy', value, 365 * 24 * 60 * 60, '/', domain );
						};

						var setCcpaAppliesCookie = function( value ) {
							var domain = '.wordpress.com' === location.hostname.slice( -14 ) ? '.rootDomain' : location.hostname;
							cookieLib.setItem( 'ccpa_applies', value, 24 * 60 * 60, '/', domain );
						}

						var maybeCallDoNotSellCallback = function() {
							if ( 'function' === typeof window.doNotSellCallback ) {
								return window.doNotSellCallback();
							}

							return false;
						}

						var usprivacyCookie = cookieLib.getItem( 'usprivacy' );

						if ( null !== usprivacyCookie ) {
							maybeCallDoNotSellCallback();
							return;
						}

						var ccpaCookie = cookieLib.getItem( 'ccpa_applies' );

						if ( null === ccpaCookie ) {

							var request = new XMLHttpRequest();
							request.open( 'GET', 'https://public-api.wordpress.com/geo/', true );

							request.onreadystatechange = function () {
								if ( 4 === this.readyState ) {
									if ( 200 === this.status ) {

										var data = JSON.parse( this.response );
										var ccpa_applies = data['region'] && data['region'].toLowerCase() === 'california';

										setCcpaAppliesCookie( ccpa_applies );

										if ( ccpa_applies ) {
											if ( maybeCallDoNotSellCallback() ) {
												setDefaultOptInCookie();
											}
										}
									} else {
										setCcpaAppliesCookie( true );
										if ( maybeCallDoNotSellCallback() ) {
											setDefaultOptInCookie();
										}
									}
								}
							};

							request.send();
						} else {
							if ( ccpaCookie === 'true' ) {
								if ( maybeCallDoNotSellCallback() ) {
									setDefaultOptInCookie();
								}
							}
						}
					} );
				};

				if ( window.defQueue && defQueue.isLOHP && defQueue.isLOHP === 2020 ) {
					defQueue.items.push( setupPrivacy );
				} else {
					setupPrivacy();
				}

			} )();
		</script>

		<!-- CCPA [end] -->
					<script type="text/javascript">
			( function( $ ) {
				$( document.body ).on( 'post-load', function () {
					if ( typeof __ATA.insertInlineAds === 'function' ) {
						__ATA.insertInlineAds();
					}
				} );
			} )( jQuery );
			</script>
<script>
window.addEventListener( "load", function( event ) {
	var link = document.createElement( "link" );
	link.href = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.css?v=20201002";
	link.type = "text/css";
	link.rel = "stylesheet";
	document.head.appendChild( link );

	var script = document.createElement( "script" );
	script.src = "https://s0.wp.com/wp-content/mu-plugins/actionbar/actionbar.js?v=20210817";
	script.defer = true;
	document.body.appendChild( script );
} );
</script>

			<div id="jp-carousel-loading-overlay">
			<div id="jp-carousel-loading-wrapper">
				<span id="jp-carousel-library-loading">&nbsp;</span>
			</div>
		</div>
		<div class="jp-carousel-overlay" style="display: none;">

		<div class="jp-carousel-container">
			<!-- The Carousel Swiper -->
			<div class="jp-carousel-wrap swiper-container jp-carousel-swiper-container jp-carousel-transitions" itemscope="" itemtype="https://schema.org/ImageGallery">
				<div class="jp-carousel swiper-wrapper"></div>
				<div class="jp-swiper-button-prev swiper-button-prev">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskPrev" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="9" height="12">
							<path d="M16.2072 16.59L11.6496 12L16.2072 7.41L14.8041 6L8.8335 12L14.8041 18L16.2072 16.59Z" fill="white"></path>
						</mask>
						<g mask="url(#maskPrev)">
							<rect x="0.579102" width="23.8823" height="24" fill="#FFFFFF"></rect>
						</g>
					</svg>
				</div>
				<div class="jp-swiper-button-next swiper-button-next">
					<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
						<mask id="maskNext" mask-type="alpha" maskUnits="userSpaceOnUse" x="8" y="6" width="8" height="12">
							<path d="M8.59814 16.59L13.1557 12L8.59814 7.41L10.0012 6L15.9718 12L10.0012 18L8.59814 16.59Z" fill="white"></path>
						</mask>
						<g mask="url(#maskNext)">
							<rect x="0.34375" width="23.8822" height="24" fill="#FFFFFF"></rect>
						</g>
					</svg>
				</div>
			</div>
			<!-- The main close buton -->
			<div class="jp-carousel-close-hint">
				<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
					<mask id="maskClose" mask-type="alpha" maskUnits="userSpaceOnUse" x="5" y="5" width="15" height="14">
						<path d="M19.3166 6.41L17.9135 5L12.3509 10.59L6.78834 5L5.38525 6.41L10.9478 12L5.38525 17.59L6.78834 19L12.3509 13.41L17.9135 19L19.3166 17.59L13.754 12L19.3166 6.41Z" fill="white"></path>
					</mask>
					<g mask="url(#maskClose)">
						<rect x="0.409668" width="23.8823" height="24" fill="#FFFFFF"></rect>
					</g>
				</svg>
			</div>
			<!-- Image info, comments and meta -->
			<div class="jp-carousel-info">
				<div class="jp-carousel-info-footer">
					<div class="jp-carousel-pagination-container">
						<div class="jp-swiper-pagination swiper-pagination"></div>
						<div class="jp-carousel-pagination"></div>
					</div>
					<div class="jp-carousel-photo-title-container">
						<h2 class="jp-carousel-photo-caption"></h2>
					</div>
					<div class="jp-carousel-photo-icons-container">
						<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#" class="jp-carousel-icon-btn jp-carousel-icon-info" aria-label="Toggle photo metadata visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskInfo" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M12.7537 2C7.26076 2 2.80273 6.48 2.80273 12C2.80273 17.52 7.26076 22 12.7537 22C18.2466 22 22.7046 17.52 22.7046 12C22.7046 6.48 18.2466 2 12.7537 2ZM11.7586 7V9H13.7488V7H11.7586ZM11.7586 11V17H13.7488V11H11.7586ZM4.79292 12C4.79292 16.41 8.36531 20 12.7537 20C17.142 20 20.7144 16.41 20.7144 12C20.7144 7.59 17.142 4 12.7537 4C8.36531 4 4.79292 7.59 4.79292 12Z" fill="white"></path>
									</mask>
									<g mask="url(#maskInfo)">
										<rect x="0.8125" width="23.8823" height="24" fill="#FFFFFF"></rect>
									</g>
								</svg>
							</span>
						</a>
												<a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/#" class="jp-carousel-icon-btn jp-carousel-icon-comments" aria-label="Toggle photo comments visibility">
							<span class="jp-carousel-icon">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="maskComments" mask-type="alpha" maskUnits="userSpaceOnUse" x="2" y="2" width="21" height="20">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M4.3271 2H20.2486C21.3432 2 22.2388 2.9 22.2388 4V16C22.2388 17.1 21.3432 18 20.2486 18H6.31729L2.33691 22V4C2.33691 2.9 3.2325 2 4.3271 2ZM6.31729 16H20.2486V4H4.3271V18L6.31729 16Z" fill="white"></path>
									</mask>
									<g mask="url(#maskComments)">
										<rect x="0.34668" width="23.8823" height="24" fill="#FFFFFF"></rect>
									</g>
								</svg>

								<span class="jp-carousel-has-comments-indicator" aria-label="This image has comments."></span>
							</span>
						</a>
											</div>
				</div>
				<div class="jp-carousel-info-extra">
					<div class="jp-carousel-info-content-wrapper">
						<div class="jp-carousel-photo-title-container">
							<h2 class="jp-carousel-photo-title"></h2>
						</div>
						<div class="jp-carousel-comments-wrapper">
															<div id="jp-carousel-comments-loading">
									<span>Loading Comments...</span>
								</div>
								<div class="jp-carousel-comments"></div>
								<div id="jp-carousel-comment-form-container">
									<span id="jp-carousel-comment-form-spinner">&nbsp;</span>
									<div id="jp-carousel-comment-post-results"></div>
																														<form id="jp-carousel-comment-form">
												<label for="jp-carousel-comment-form-comment-field" class="screen-reader-text">Write a Comment...</label>
												<textarea name="comment" class="jp-carousel-comment-form-field jp-carousel-comment-form-textarea" id="jp-carousel-comment-form-comment-field" placeholder="Write a Comment..."></textarea>
												<div id="jp-carousel-comment-form-submit-and-info-wrapper">
													<div id="jp-carousel-comment-form-commenting-as">
																													<fieldset>
																<label for="jp-carousel-comment-form-email-field">Email (Required)</label>
																<input type="text" name="email" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-email-field">
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-author-field">Name (Required)</label>
																<input type="text" name="author" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-author-field">
															</fieldset>
															<fieldset>
																<label for="jp-carousel-comment-form-url-field">Website</label>
																<input type="text" name="url" class="jp-carousel-comment-form-field jp-carousel-comment-form-text-field" id="jp-carousel-comment-form-url-field">
															</fieldset>
																											</div>
													<input type="submit" name="submit" class="jp-carousel-comment-form-button" id="jp-carousel-comment-form-button-submit" value="Post Comment">
												</div>
											</form>
																											</div>
													</div>
						<div class="jp-carousel-image-meta">
							<div class="jp-carousel-title-and-caption">
								<div class="jp-carousel-photo-info">
									<h3 class="jp-carousel-caption" itemprop="caption description"></h3>
								</div>

								<div class="jp-carousel-photo-description"></div>
							</div>
							<ul class="jp-carousel-image-exif" style="display: none;"></ul>
							<a class="jp-carousel-image-download" target="_blank" style="display: none;">
								<svg width="25" height="24" viewBox="0 0 25 24" fill="none" xmlns="http://www.w3.org/2000/svg">
									<mask id="mask0" mask-type="alpha" maskUnits="userSpaceOnUse" x="3" y="3" width="19" height="18">
										<path fill-rule="evenodd" clip-rule="evenodd" d="M5.84615 5V19H19.7775V12H21.7677V19C21.7677 20.1 20.8721 21 19.7775 21H5.84615C4.74159 21 3.85596 20.1 3.85596 19V5C3.85596 3.9 4.74159 3 5.84615 3H12.8118V5H5.84615ZM14.802 5V3H21.7677V10H19.7775V6.41L9.99569 16.24L8.59261 14.83L18.3744 5H14.802Z" fill="white"></path>
									</mask>
									<g mask="url(#mask0)">
										<rect x="0.870605" width="23.8823" height="24" fill="#FFFFFF"></rect>
									</g>
								</svg>
								<span class="jp-carousel-download-text"></span>
							</a>
							<div class="jp-carousel-image-map" style="display: none;"></div>
						</div>
					</div>
				</div>
			</div>
		</div>

		</div>
		
	<script type="text/javascript">
		window.WPCOM_sharing_counts = {"https:\/\/studywolf.wordpress.com\/2013\/07\/01\/reinforcement-learning-sarsa-vs-q-learning\/":1043};
	</script>
				<script crossorigin="anonymous" type="text/javascript" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/saved_resource(7)"></script>
<script type="text/javascript">
	(function(){
		var corecss = document.createElement('link');
		var themecss = document.createElement('link');
		var corecssurl = "https://s0.wp.com/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shCore.css?ver=3.0.9b";
		if ( corecss.setAttribute ) {
				corecss.setAttribute( "rel", "stylesheet" );
				corecss.setAttribute( "type", "text/css" );
				corecss.setAttribute( "href", corecssurl );
		} else {
				corecss.rel = "stylesheet";
				corecss.href = corecssurl;
		}
		document.head.appendChild( corecss );
		var themecssurl = "https://s0.wp.com/wp-content/plugins/syntaxhighlighter/syntaxhighlighter3/styles/shThemeDefault.css?m=1363304414h&amp;ver=3.0.9b";
		if ( themecss.setAttribute ) {
				themecss.setAttribute( "rel", "stylesheet" );
				themecss.setAttribute( "type", "text/css" );
				themecss.setAttribute( "href", themecssurl );
		} else {
				themecss.rel = "stylesheet";
				themecss.href = themecssurl;
		}
		document.head.appendChild( themecss );
	})();
	SyntaxHighlighter.config.strings.expandSource = '+ expand source';
	SyntaxHighlighter.config.strings.help = '?';
	SyntaxHighlighter.config.strings.alert = 'SyntaxHighlighter\n\n';
	SyntaxHighlighter.config.strings.noBrush = 'Can\'t find brush for: ';
	SyntaxHighlighter.config.strings.brushNotHtmlScript = 'Brush wasn\'t configured for html-script option: ';
	SyntaxHighlighter.defaults['pad-line-numbers'] = false;
	SyntaxHighlighter.defaults['toolbar'] = false;
	SyntaxHighlighter.all();

	// Infinite scroll support
	if ( typeof( jQuery ) !== 'undefined' ) {
		jQuery( function( $ ) {
			$( document.body ).on( 'post-load', function() {
				SyntaxHighlighter.highlight();
			} );
		} );
	}
</script>
<link rel="stylesheet" id="all-css-0-3" href="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/saved_resource(8)" type="text/css" media="all">
<script id="jetpack-carousel-js-extra">
var jetpackSwiperLibraryPath = {"url":"\/wp-content\/mu-plugins\/carousel\/swiper-bundle.js"};
var jetpackCarouselStrings = {"widths":[370,700,1000,1200,1400,2000],"is_logged_in":"","lang":"en","ajaxurl":"https:\/\/studywolf.wordpress.com\/wp-admin\/admin-ajax.php","nonce":"72e022e3cf","display_exif":"1","display_comments":"1","display_geo":"1","single_image_gallery":"1","single_image_gallery_media_file":"","background_color":"black","comment":"Comment","post_comment":"Post Comment","write_comment":"Write a Comment...","loading_comments":"Loading Comments...","download_original":"View full size <span class=\"photo-size\">{0}<span class=\"photo-size-times\">\u00d7<\/span>{1}<\/span>","no_comment_text":"Please be sure to submit some text with your comment.","no_comment_email":"Please provide an email address to comment.","no_comment_author":"Please provide your name to comment.","comment_post_error":"Sorry, but there was an error posting your comment. Please try again later.","comment_approved":"Your comment was approved.","comment_unapproved":"Your comment is in moderation.","camera":"Camera","aperture":"Aperture","shutter_speed":"Shutter Speed","focal_length":"Focal Length","copyright":"Copyright","comment_registration":"0","require_name_email":"1","login_url":"https:\/\/studywolf.wordpress.com\/wp-login.php?redirect_to=https%3A%2F%2Fstudywolf.wordpress.com%2F2013%2F07%2F01%2Freinforcement-learning-sarsa-vs-q-learning%2F","blog_id":"39795907","meta_data":["camera","aperture","shutter_speed","focal_length","copyright"],"stats_query_args":"blog=39795907&v=wpcom&tz=-5&user_id=0&subd=studywolf","is_public":"1"};
</script>
<script id="sharing-js-js-extra">
var sharing_js_options = {"lang":"en","counts":"1","is_stats_active":"1"};
</script>
<script async="true" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/saved_resource(9)"></script><script crossorigin="anonymous" type="text/javascript" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/saved_resource(10)"></script>
<script type="text/javascript">
var windowOpen;
			( function () {
				function matches( el, sel ) {
					return !! (
						el.matches && el.matches( sel ) ||
						el.msMatchesSelector && el.msMatchesSelector( sel )
					);
				}

				document.body.addEventListener( 'click', function ( event ) {
					if ( ! event.target ) {
						return;
					}

					var el;
					if ( matches( event.target, 'a.share-twitter' ) ) {
						el = event.target;
					} else if ( event.target.parentNode && matches( event.target.parentNode, 'a.share-twitter' ) ) {
						el = event.target.parentNode;
					}

					if ( el ) {
						event.preventDefault();

						// If there's another sharing window open, close it.
						if ( typeof windowOpen !== 'undefined' ) {
							windowOpen.close();
						}
						windowOpen = window.open( el.getAttribute( 'href' ), 'wpcomtwitter', 'menubar=1,resizable=1,width=600,height=350' );
						return false;
					}
				} );
			} )();
var windowOpen;
			( function () {
				function matches( el, sel ) {
					return !! (
						el.matches && el.matches( sel ) ||
						el.msMatchesSelector && el.msMatchesSelector( sel )
					);
				}

				document.body.addEventListener( 'click', function ( event ) {
					if ( ! event.target ) {
						return;
					}

					var el;
					if ( matches( event.target, 'a.share-facebook' ) ) {
						el = event.target;
					} else if ( event.target.parentNode && matches( event.target.parentNode, 'a.share-facebook' ) ) {
						el = event.target.parentNode;
					}

					if ( el ) {
						event.preventDefault();

						// If there's another sharing window open, close it.
						if ( typeof windowOpen !== 'undefined' ) {
							windowOpen.close();
						}
						windowOpen = window.open( el.getAttribute( 'href' ), 'wpcomfacebook', 'menubar=1,resizable=1,width=600,height=400' );
						return false;
					}
				} );
			} )();
</script>
<script type="text/javascript">
// <![CDATA[
(function() {
try{
  if ( window.external &&'msIsSiteMode' in window.external) {
    if (window.external.msIsSiteMode()) {
      var jl = document.createElement('script');
      jl.type='text/javascript';
      jl.async=true;
      jl.src='/wp-content/plugins/ie-sitemode/custom-jumplist.php';
      var s = document.getElementsByTagName('script')[0];
      s.parentNode.insertBefore(jl, s);
    }
  }
}catch(e){}
})();
// ]]>
</script>	<iframe src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/master.html" scrolling="no" id="likes-master" name="likes-master" style="display:none;"></iframe>
	<div id="likes-other-gravatars">
		<div class="likes-text">
			<span>%d</span> bloggers like this:		</div>
		<ul class="wpl-avatars sd-like-gravatars"></ul>
	</div>
	
		<script src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/w.js.téléchargé" defer=""></script> <script type="text/javascript">
_tkq = window._tkq || [];
_stq = window._stq || [];
_tkq.push(['storeContext', {'blog_id':'39795907','blog_tz':'-5','user_lang':'en','blog_lang':'en','user_id':'0'}]);
_stq.push(['view', {'blog':'39795907','v':'wpcom','tz':'-5','user_id':'0','post':'1043','subd':'studywolf'}]);
_stq.push(['extra', {'crypt':'UE5XaGUuOTlwaD85flAmcm1mcmZsaDhkV11YdWFnNncxc1tjZG9XVXhRZS03JixXRU5XS218Ul8rcWglJXN8LCVjM0xGZktTRHA5a1lCTVFwLFFFWmZ+WTdBM0k1NC9zeXBsTWxmeERWd1ozJXhweHcsYT1aaT82TndyczRrSFdjRm9YRDhlfEh4UmldYS5sP25zYndud34/MmNKYmRlc1pYUVUtZS1VUFVKMmRWZkYrNS14bG5EOEJIdko3MHBFanhPdW1ZYysmTldmWWcrS1hKYTNJUzh2Q2YzOElJaHVscSZwcl8wbGlUc1dibnlzYlpvb0F5bENqdXRnZXJEZjhMTHM5NURi'}]);
_stq.push([ 'clickTrackerInit', '39795907', '1043' ]);
	</script>
<noscript><img src="https://pixel.wp.com/b.gif?v=noscript" style="height:1px;width:1px;overflow:hidden;position:absolute;bottom:1px;" alt="" /></noscript>
<script>
if ( 'object' === typeof wpcom_mobile_user_agent_info ) {

	wpcom_mobile_user_agent_info.init();
	var mobileStatsQueryString = "";
	
	if( false !== wpcom_mobile_user_agent_info.matchedPlatformName )
		mobileStatsQueryString += "&x_" + 'mobile_platforms' + '=' + wpcom_mobile_user_agent_info.matchedPlatformName;
	
	if( false !== wpcom_mobile_user_agent_info.matchedUserAgentName )
		mobileStatsQueryString += "&x_" + 'mobile_devices' + '=' + wpcom_mobile_user_agent_info.matchedUserAgentName;
	
	if( wpcom_mobile_user_agent_info.isIPad() )
		mobileStatsQueryString += "&x_" + 'ipad_views' + '=' + 'views';

	if( "" != mobileStatsQueryString ) {
		new Image().src = document.location.protocol + '//pixel.wp.com/g.gif?v=wpcom-no-pv' + mobileStatsQueryString + '&baba=' + Math.random();
	}
	
}
</script>

<iframe name="__tcfapiLocator" style="display: none;" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/saved_resource(12).html"></iframe><script src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/actionbar.js.téléchargé" defer=""></script><div id="actionbar" class="actnbr-pub-chunk actnbr-has-follow"><ul><li class="actnbr-btn actnbr-hidden"> 				<a class="actnbr-action actnbr-actn-follow" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/"><svg class="gridicon gridicon__follow" height="24px" width="24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><g><path d="M23 16v2h-3v3h-2v-3h-3v-2h3v-3h2v3h3zM20 2v9h-4v3h-3v4H4c-1.1 0-2-.9-2-2V2h18zM8 13v-1H4v1h4zm3-3H4v1h7v-1zm0-2H4v1h7V8zm7-4H4v2h14V4z"></path></g></svg><span>Follow</span></a><a class="actnbr-action actnbr-actn-following no-display" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/"><svg class="gridicon gridicon__following" height="24px" width="24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><g><path d="M23 13.482L15.508 21 12 17.4l1.412-1.388 2.106 2.188 6.094-6.094L23 13.482zm-7.455 1.862L20 10.89V2H2v14c0 1.1.9 2 2 2h4.538l4.913-4.832 2.095 2.176zM8 13H4v-1h4v1zm3-2H4v-1h7v1zm0-2H4V8h7v1zm7-3H4V4h14v2z"></path></g></svg><span>Following</span></a> 				<div class="actnbr-popover tip tip-top-left actnbr-notice"> 					<div class="tip-arrow"></div> 					<div class="tip-inner actnbr-follow-bubble"> 				<ul> 					<li class="actnbr-sitename actnbr-hidden"><a href="http://studywolf.wordpress.com/"><img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/wpcom-gray-white.png" class="avatar avatar-50" height="50" width="50"> studywolf</a></li> 					<form> 					<li class="actnbr-login-nudge actnbr-hidden"><div>Already have a WordPress.com account? <a href="https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstudywolf.wordpress.com%2F2013%2F07%2F01%2Freinforcement-learning-sarsa-vs-q-learning%2F&amp;signup_flow=account">Log in now.</a></div></li> 				</form></ul> 			</div> 				</div> 					</li><li class="actnbr-ellipsis actnbr-hidden"> 				<svg class="gridicon gridicon__ellipsis" height="24" width="24" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><g><circle cx="5" cy="12" r="2"></circle><circle cx="19" cy="12" r="2"></circle><circle cx="12" cy="12" r="2"></circle></g></svg> 				<div class="actnbr-popover tip tip-top-left actnbr-more"> 				<div class="tip-arrow"></div> 				<div class="tip-inner"> 					<ul> 						<li class="actnbr-sitename actnbr-hidden"><a href="http://studywolf.wordpress.com/"><img alt="" src="./Reinforcement Learning part 2_ SARSA vs Q-learning _ studywolf_files/wpcom-gray-white.png" class="avatar avatar-50" height="50" width="50"> studywolf</a></li> 					<li class="actnbr-folded-customize actnbr-hidden"><a href="https://studywolf.wordpress.com/wp-admin/customize.php?url=https%3A%2F%2Fstudywolf.wordpress.com%2F2013%2F07%2F01%2Freinforcement-learning-sarsa-vs-q-learning%2F"><svg class="gridicon gridicon__customize" height="20px" width="20px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><g><path d="M2 6c0-1.505.78-3.08 2-4 0 .845.69 2 2 2 1.657 0 3 1.343 3 3 0 .386-.08.752-.212 1.09.74.594 1.476 1.19 2.19 1.81L8.9 11.98c-.62-.716-1.214-1.454-1.807-2.192C6.753 9.92 6.387 10 6 10c-2.21 0-4-1.79-4-4zm12.152 6.848l1.34-1.34c.607.304 1.283.492 2.008.492 2.485 0 4.5-2.015 4.5-4.5 0-.725-.188-1.4-.493-2.007L18 9l-2-2 3.507-3.507C18.9 3.188 18.225 3 17.5 3 15.015 3 13 5.015 13 7.5c0 .725.188 1.4.493 2.007L3 20l2 2 6.848-6.848c1.885 1.928 3.874 3.753 5.977 5.45l1.425 1.148 1.5-1.5-1.15-1.425c-1.695-2.103-3.52-4.092-5.448-5.977z" data-reactid=".2.1.1:0.1b.0"></path></g></svg><span>Customize<span></span></span></a></li> 						<li class="actnbr-folded-follow actnbr-hidden"><a class="actnbr-action actnbr-actn-follow" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/"><svg class="gridicon gridicon__follow" height="24px" width="24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><g><path d="M23 16v2h-3v3h-2v-3h-3v-2h3v-3h2v3h3zM20 2v9h-4v3h-3v4H4c-1.1 0-2-.9-2-2V2h18zM8 13v-1H4v1h4zm3-3H4v1h7v-1zm0-2H4v1h7V8zm7-4H4v2h14V4z"></path></g></svg><span>Follow</span></a><a class="actnbr-action actnbr-actn-following no-display" href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/"><svg class="gridicon gridicon__following" height="24px" width="24px" xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><g><path d="M23 13.482L15.508 21 12 17.4l1.412-1.388 2.106 2.188 6.094-6.094L23 13.482zm-7.455 1.862L20 10.89V2H2v14c0 1.1.9 2 2 2h4.538l4.913-4.832 2.095 2.176zM8 13H4v-1h4v1zm3-2H4v-1h7v1zm0-2H4V8h7v1zm7-3H4V4h14v2z"></path></g></svg><span>Following</span></a></li> 					<li class="actnbr-signup actnbr-hidden"><a href="https://wordpress.com/start/">Sign up</a></li> 						<li class="actnbr-login actnbr-hidden"><a href="https://wordpress.com/log-in?redirect_to=https%3A%2F%2Fstudywolf.wordpress.com%2F2013%2F07%2F01%2Freinforcement-learning-sarsa-vs-q-learning%2F&amp;signup_flow=account">Log in</a></li> 						 						<li class="actnbr-shortlink actnbr-hidden"><a href="https://wp.me/p2GYJt-gP">Copy shortlink</a></li> 						<li class="flb-report actnbr-hidden"><a href="http://en.wordpress.com/abuse/">Report this content</a></li> 						 						 						<li class="actnbr-subs actnbr-hidden"><a href="https://subscribe.wordpress.com/">Manage subscriptions</a></li> 						<li class="actnbr-fold actnbr-hidden"><a href="https://studywolf.wordpress.com/2013/07/01/reinforcement-learning-sarsa-vs-q-learning/">Collapse this bar</a></li> 						</ul> 					</div> 					</div> 				</li> 				</ul></div></body></html>